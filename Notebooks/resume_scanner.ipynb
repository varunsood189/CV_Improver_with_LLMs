{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "304d4346-3cf6-488c-ab9a-39e4561a15dc",
      "metadata": {
        "id": "304d4346-3cf6-488c-ab9a-39e4561a15dc"
      },
      "source": [
        "# 🔎 Resume scanner: 🚀 Leverage the power of LLM to improve your resume\n",
        "### 🚀 Build a Streamlit application powered by Langchain, OpenAI and Google Generative AI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18f8d481",
      "metadata": {
        "id": "18f8d481"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Introduction](#intro)<br>\n",
        "2. [Setup](#setup)<br>\n",
        "    2.1. [Import](#import)<br>\n",
        "    2.2. [API keys](#apikeys)<br>\n",
        "3. [Retrieval](#retrieval)<br>\n",
        "    3.1. [Upload resume](#uploadResume)<br>\n",
        "    3.2. [Token Count](#tokenCount)<br>\n",
        "    3.3. [Embeddings](#embeddings)<br>\n",
        "    3.4. [Vectorstores](#vectorstores)<br>\n",
        "    3.5. [Vectorstore-backed retriever](#retriever)<br>\n",
        "    3.6. [Cohere reranker](#cohere)<br>\n",
        "4. [Instantiate LLMs](#llm)<br>    \n",
        "5. [Resume Scanner](#scanner)<br>\n",
        "    5.1  [Prompt Templates](#prompt_templates)<br>\n",
        "    5.2. [Invoke LLMs](#invoke_llms)<br>\n",
        "    5.3. [Contact Information](#contact_info)<br>\n",
        "    5.4. [Summary](#summary)<br>\n",
        "    5.5. [Education and Language](#edu_n_language)<br>\n",
        "    5.6. [Skills and  Certifications](#skills_certifs)<br>\n",
        "    5.7. [Work experience and Projects](#professional)<br>\n",
        "    5.8. [Work experience responsibilities and project details](#professional_details)<br>\n",
        "    5.9. [Improve the work experience section](#improve_workExp)<br>\n",
        "    5.10. [Improve the projects section](#improve_projects)<br>\n",
        "    5.11. [Evaluate the Resume](#evaluate_resume)<br>\n",
        "    5.12. [Putting it all together](#recap)<br>\n",
        "7. [Streamlit application](#app)<br>\n",
        "8. [Concluding](#conclusion)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f859e2-b019-4454-9ee2-e22a4f5e10fe",
      "metadata": {
        "id": "50f859e2-b019-4454-9ee2-e22a4f5e10fe"
      },
      "source": [
        "# <a class=\"anchor\" id=\"intro\">Introduction</a>\n",
        "\n",
        "This project aims to build a WEB application in [Streamlit](https://streamlit.io/) that will scan and improve a resume using instruction-tuned Large Language Models (LLMs).\n",
        "\n",
        "This is how the app should work: The first step is to upload a CV in PDF format. The app will then extract all the information, such as contact details, summary, work experience and skills. It will assess the quality of each section and return a score from 0 to 100. The app will also suggest improvements to make the text more appealing to recruiters.\n",
        "\n",
        "We will leverage the power of LLMs, specifically Chat GPT from [OpenAI](https://platform.openai.com/overview) and Gemini-pro from [Google](https://ai.google.dev/?hl=en), to extract, assess, and enhance resumes\n",
        ".\n",
        "We will use [Langchain](https://python.langchain.com/docs/get_started/introduction), prompt engineering, and retrieval augmented generation techniques to complete these steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef2b25d7",
      "metadata": {
        "id": "ef2b25d7"
      },
      "source": [
        "# <a class=\"anchor\" id=\"setup\">Setup</a>\n",
        "\n",
        "Before we get started, we need to import the relevant Python libraries and LLM API keys."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4372f9d-4d97-4849-8ab9-3ee12170b067",
      "metadata": {
        "id": "f4372f9d-4d97-4849-8ab9-3ee12170b067"
      },
      "source": [
        "## <a class=\"anchor\" id=\"import\">Import libraries</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f8d50a31-4aa0-4b89-b2a1-c47d79884dc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f8d50a31-4aa0-4b89-b2a1-c47d79884dc9",
        "outputId": "e0c4b761-3151-4dc0-83ea-ca4765b4313d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-cohere\n",
            "  Downloading langchain_cohere-0.3.4-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting cohere<6.0,>=5.5.6 (from langchain-cohere)\n",
            "  Downloading cohere-5.13.8-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (0.3.29)\n",
            "Collecting langchain-experimental<0.4.0,>=0.3.0 (from langchain-cohere)\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (2.10.5)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere) (0.9.0)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (0.28.1)\n",
            "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (0.21.0)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (0.2.10)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (9.0.0)\n",
            "Collecting langchain-community<0.4.0,>=0.3.0 (from langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.3->langchain-cohere) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.3->langchain-cohere) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.3->langchain-cohere) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.3->langchain-cohere) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-cohere) (0.7.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-cohere) (3.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (3.11.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (0.3.14)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-cohere) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-cohere) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.3->langchain-cohere) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5.6->langchain-cohere) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5.6->langchain-cohere) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (0.27.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading marshmallow-3.25.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (2024.10.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (4.67.1)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (0.3.5)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere) (3.1.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (1.3.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_cohere-0.3.4-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.13.8-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.25.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: types-requests, python-dotenv, parameterized, mypy-extensions, marshmallow, httpx-sse, fastavro, typing-inspect, pydantic-settings, dataclasses-json, cohere, langchain-community, langchain-experimental, langchain-cohere\n",
            "Successfully installed cohere-5.13.8 dataclasses-json-0.6.7 fastavro-1.10.0 httpx-sse-0.4.0 langchain-cohere-0.3.4 langchain-community-0.3.14 langchain-experimental-0.3.4 marshmallow-3.25.1 mypy-extensions-1.0.0 parameterized-0.9.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 types-requests-2.32.0.20241016 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.29)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.0.9-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.8.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.29)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.10.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.25.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.25.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.2.10)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.69.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.9-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-2.0.9\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.30 (from langchain-openai)\n",
            "  Downloading langchain_core-0.3.30-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.59.6)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain-openai) (0.2.10)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain-openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain-openai) (2.10.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.30->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.30->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.30->langchain-openai) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.30->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.30->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.30->langchain-openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.1-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.30-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.9/411.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.29\n",
            "    Uninstalling langchain-core-0.3.29:\n",
            "      Successfully uninstalled langchain-core-0.3.29\n",
            "Successfully installed langchain-core-0.3.30 langchain-openai-0.3.1 tiktoken-0.8.0\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.30)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.25.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (2.10.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.11/dist-packages (5.13.8)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.4.0)\n",
            "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.9.0)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.10.5)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.21.0)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.0.20241016)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere) (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting tiktoken==0.5.2\n",
            "  Downloading tiktoken-0.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.5.2) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.5.2) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.5.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.5.2) (2024.12.14)\n",
            "Downloading tiktoken-0.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.8.0\n",
            "    Uninstalling tiktoken-0.8.0:\n",
            "      Successfully uninstalled tiktoken-0.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-openai 0.3.1 requires tiktoken<1,>=0.7, but you have tiktoken 0.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (3.7)\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n",
            "Collecting streamlit==1.28.0\n",
            "  Downloading streamlit-1.28.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (8.1.8)\n",
            "Collecting importlib-metadata<7,>=1.4 (from streamlit==1.28.0)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (1.26.4)\n",
            "Collecting packaging<24,>=16.8 (from streamlit==1.28.0)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (2.2.2)\n",
            "Collecting pillow<11,>=7.1.0 (from streamlit==1.28.0)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (17.0.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (13.9.4)\n",
            "Collecting tenacity<9,>=8.1.0 (from streamlit==1.28.0)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (4.12.2)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit==1.28.0)\n",
            "  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit==1.28.0)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.28.0) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit==1.28.0)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.28.0) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.28.0) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit==1.28.0) (1.21.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.28.0) (4.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<7,>=1.4->streamlit==1.28.0) (3.21.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.3.0->streamlit==1.28.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.3.0->streamlit==1.28.0) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.3->streamlit==1.28.0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.28.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.28.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.28.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit==1.28.0) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit==1.28.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit==1.28.0) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.28.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit==1.28.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.0) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.0) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.28.0) (0.1.2)\n",
            "Downloading streamlit-1.28.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading validators-0.34.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, validators, tenacity, pillow, packaging, importlib-metadata, pydeck, streamlit\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.5.0\n",
            "    Uninstalling importlib_metadata-8.5.0:\n",
            "      Successfully uninstalled importlib_metadata-8.5.0\n",
            "Successfully installed importlib-metadata-6.11.0 packaging-23.2 pillow-10.4.0 pydeck-0.9.1 streamlit-1.28.0 tenacity-8.5.0 validators-0.34.0 watchdog-6.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "importlib_metadata"
                ]
              },
              "id": "29d552c9377348b1ba53ce15b9955d00"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain-cohere\n",
        "!pip install langchain\n",
        "!pip install langchain-google-genai\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-community\n",
        "!pip install cohere\n",
        "!pip install tiktoken==0.5.2\n",
        "!pip install Pillow\n",
        "!pip install faiss-cpu\n",
        "!pip install python-dotenv\n",
        "!pip install markdown\n",
        "!pip install pdfminer.six\n",
        "!pip install streamlit==1.28.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Ab8Pg7NM9UA6",
      "metadata": {
        "id": "Ab8Pg7NM9UA6"
      },
      "outputs": [],
      "source": [
        "# from google.colab import userdata\n",
        "\n",
        "# cohere_api_key = userdata.get('cohere_api_key')\n",
        "# from langchain_cohere import CohereEmbeddings\n",
        "# cohere_embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\",\n",
        "#             cohere_api_key=cohere_api_key)\n",
        "# text = \"This is a test document.\"\n",
        "\n",
        "# query_result = cohere_embeddings.embed_query(text)\n",
        "# print(query_result)\n",
        "\n",
        "# doc_result = cohere_embeddings.embed_documents([text])\n",
        "# print(doc_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "249e8a6b-fc11-4653-9cfe-7687bac346ca",
      "metadata": {
        "id": "249e8a6b-fc11-4653-9cfe-7687bac346ca"
      },
      "outputs": [],
      "source": [
        "# Langchain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain_community.document_loaders import PDFMinerLoader, TextLoader, Docx2txtLoader\n",
        "\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Other libraries\n",
        "import datetime, json, os, tiktoken\n",
        "from IPython.display import Markdown\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "eaa62c11-f27d-4be2-815d-0e3e0b560285",
      "metadata": {
        "id": "eaa62c11-f27d-4be2-815d-0e3e0b560285"
      },
      "outputs": [],
      "source": [
        "# Feel free to update the assistant language.\n",
        "ASSISTAN_LANGUAGE = \"english\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33214148-b61f-4d63-9385-7b48c3fd8eb6",
      "metadata": {
        "id": "33214148-b61f-4d63-9385-7b48c3fd8eb6"
      },
      "source": [
        "## <a class=\"anchor\" id=\"apikeys\">API Keys</a>\n",
        "\n",
        "Our application requires API keys from OpenAI, Google and Cohere.\n",
        "- **OpenAI** API key: [Get an API key](https://platform.openai.com/account/api-keys)\n",
        "- **Google** API key: [Get an API key](https://makersuite.google.com/app/apikey)\n",
        "- **Cohere** API key: [Get an API key](https://dashboard.cohere.com/api-keys)\n",
        "\n",
        "For security reasons, We will **NOT include** our secret keys in the notebook.\n",
        "\n",
        "We will store them in a `.env` file and use the [dotenv](https://pypi.org/project/python-dotenv/) library to read them and set them as environment variables."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9f3950d-46cd-49f7-9110-0190832e3364",
      "metadata": {
        "id": "b9f3950d-46cd-49f7-9110-0190832e3364"
      },
      "source": [
        "**Please add your API keys to the `keys.env` file before running the next cell.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1f1eb275-e738-42b7-90c2-255b243a2607",
      "metadata": {
        "id": "1f1eb275-e738-42b7-90c2-255b243a2607"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "try:\n",
        "    found_dotenv = find_dotenv('keys.env',usecwd=True)\n",
        "    load_dotenv(found_dotenv)\n",
        "\n",
        "    openai_api_key = os.getenv(\"api_key_openai\")\n",
        "    google_api_key = os.getenv(\"api_key_google\")\n",
        "    cohere_api_key = os.getenv(\"api_key_cohere\")\n",
        "\n",
        "    if (openai_api_key==\"Your_API_key\") or (google_api_key==\"Your_API_key\") or (cohere_api_key==\"Your_API_key\"):\n",
        "        print(\"Please add your API keys to the keys.env file\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd2980f",
      "metadata": {
        "id": "6fd2980f"
      },
      "source": [
        "# <a class=\"anchor\" id=\"retrieval\">Retrieval</a>\n",
        "\n",
        "Before analysing the resume, let's create a Langchain [retrieval](https://python.langchain.com/docs/modules/data_connection/), which includes document loaders to upload the resume, embeddings to create a numerical representation of the text, vector stores to store the embeddings, and retrievers to find the most similar documents to the query."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "153590c3-2302-4ee9-94b3-a3399ccda017",
      "metadata": {
        "id": "153590c3-2302-4ee9-94b3-a3399ccda017"
      },
      "source": [
        "## <a class=\"anchor\" id=\"uploadResume\">Upload resume</a>\n",
        "\n",
        "You can upload a resume in PDF format.\n",
        "\n",
        "We will use [PDFMinerLoader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf#using-pdfminer) from Langchain to load and split the resume into an array of documents, where each document contains the page content and the source file as metadata.\n",
        "\n",
        "For demonstration purposes, we will use a simple data scientist resume generated by ChatGPT. Feel free to upload your CV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "850712d9-1a91-415a-83f2-3e92eb078469",
      "metadata": {
        "id": "850712d9-1a91-415a-83f2-3e92eb078469"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PDFMinerLoader\n",
        "\n",
        "def langchain_document_loader(file_path):\n",
        "    \"\"\"Load and split a PDF file in Langchain.\n",
        "    Parameters:\n",
        "        - file_path (str): path of the file.\n",
        "    Output:\n",
        "        - documents: list of Langchain Documents.\"\"\"\n",
        "\n",
        "    if file_path.endswith(\".pdf\"):\n",
        "        loader = PDFMinerLoader(file_path=file_path)\n",
        "    else:\n",
        "        print(\"You can only upload .pdf files!\")\n",
        "\n",
        "    # 1. Load and split documents\n",
        "    documents = loader.load_and_split()\n",
        "\n",
        "    # 2. Update the metadata: add document number to metadata\n",
        "    for i in range(len(documents)):\n",
        "        documents[i].metadata = {\n",
        "            \"source\": documents[i].metadata[\"source\"],\n",
        "            \"doc_number\": i,\n",
        "        }\n",
        "\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbb677a-6ac0-486c-a088-f1a0a402b724",
      "metadata": {
        "id": "cfbb677a-6ac0-486c-a088-f1a0a402b724"
      },
      "source": [
        "For longer resumes, it is possible to have multiple documents. In such cases, the relevant information can be found in two consecutive documents.\n",
        "\n",
        "To ensure that the subsequent document is added to the most relevant retrieved document, we have included the document number in the metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1d3ce79f-4364-4e3b-bcf9-9638c46ac1ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d3ce79f-4364-4e3b-bcf9-9638c46ac1ec",
        "outputId": "73f1e7ee-18a9-4f0e-dfe4-e0f23ca927be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of documents: 1\n",
            "[Document(metadata={'source': '/content/data/resume/ChatGPT_dataScientist.pdf', 'doc_number': 0}, page_content='Name Candidate \\nAddress AAA \\ntest@gmail.com \\n123456789 \\n\\nObjective: \\nHighly skilled and experienced data scientist with a track record of success in both Company \\nX and Company Y. Proficient in a range of programming languages and tools, with a strong \\nability to lead teams and adapt to new challenges. Seeking opportunities to leverage \\nexpertise in data science to drive innovation and business growth. \\n\\nExperience: \\nData Scientist - Company Y (2019 - 2024) \\n\\n-  Led a team of data scientists in developing machine learning models for predictive \\n\\nanalytics. \\n\\n-  Utilized TensorFlow and PyTorch for deep learning projects, achieving significant \\n\\n- \\n\\nimprovements in model accuracy. \\nImplemented Spark and Spark MLlib for big data processing, optimizing performance \\nand scalability. \\n\\n-  Conducted data analysis using Tableau and Plotly, generating actionable insights for \\n\\nbusiness stakeholders. \\n\\n-  Collaborated with cross-functional teams to deploy machine learning solutions into \\n\\nproduction environments. \\n\\n-  Mentored junior team members, fostering their professional growth and development. \\n\\nData Scientist - Company X (2014 - 2019) \\n\\n-  Spearheaded data science initiatives, focusing on improving customer engagement \\n\\nand retention. \\n\\n-  Employed Python, Pandas, and Scikit-learn for data preprocessing and predictive \\n\\nmodeling tasks. \\n\\n-  Managed MySQL and Oracle databases, ensuring data integrity and accessibility for \\n\\nanalysis. \\n\\n-  Developed web applications using Flask and HTML to visualize and interact with \\n\\ndata. \\n\\n-  Served as a team leader, coordinating project timelines and priorities to meet \\n\\nbusiness objectives. \\n\\n-  Demonstrated adaptability in rapidly evolving business environments, delivering \\n\\nsolutions on time and within budget. \\n\\nEducation: \\nMaster of Science in Data Science \\nUniversity U1 \\n2014 \\n\\nSkills: \\nProgramming languages: Python, Java \\nData science: TensorFlow, PyTorch, Spark, Spark MLlib, Pandas, Scikit-learn \\nData analysis: Tableau, Plotly \\n\\n \\n \\n \\n \\n \\n\\x0cDatabases: MySQL, Oracle \\nWeb: Flask, HTML \\nSoft skills: Team leadership, Adaptability \\n\\nCertifications: \\nTensorFlow Developer Certificate (2022, Expires: 2025) \\n\\nRecent Projects: \\nChat with Your Data Using Retrieval Augmented Generation \\n\\n- \\n\\nImplemented a conversational AI system using retrieval augmented generation \\ntechniques. \\n\\n-  Leveraged natural language processing models to enable users to interact with data \\n\\nthrough chat interfaces. \\n\\n-  Enhanced user experience and accessibility of data-driven insights. \\n\\nImprove Resume Using the Power of LLM \\n\\n-  Applied large language model (LLM) techniques to optimize and personalize resume \\n\\ncontent. \\n\\n-  Utilized advanced natural language processing algorithms to highlight key skills and \\n\\nachievements effectively. \\n\\n- \\n\\nLanguages: \\nFluent in French, English, and Spanish.')]\n"
          ]
        }
      ],
      "source": [
        "documents = langchain_document_loader(\"/content/data/resume/ChatGPT_dataScientist.pdf\")\n",
        "print(\"number of documents:\",len(documents))\n",
        "print(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d770c76-ab94-4f67-aef2-6c5362c39aed",
      "metadata": {
        "id": "9d770c76-ab94-4f67-aef2-6c5362c39aed"
      },
      "source": [
        "## <a class=\"anchor\" id=\"tokenCount\">Token Count</a>\n",
        "\n",
        "**Token count is the main constraint for our application.**\n",
        "\n",
        "Although the input token limit is sufficient to process a standard resume (one to two pages) in one call, the output limit is smaller. GPT-3-5 and GPT-4 have an output token limit of 4096 tokens, while Gemini Pro has a limit of only 2048 tokens. For longer resumes, it is not possible to extract all information in one go. We will make multiple calls to LLM and retrieve information section by section (contact information, summary, work experience...).\n",
        "\n",
        "\n",
        "|  Provider   |  Model | maximum token limit | Output token limit | Price\n",
        "| -------- | ------- | ------- | ------- | -------\n",
        "| OpenAI  | [gpt-3.5-turbo-0125](https://platform.openai.com/docs/models/gpt-3-5-turbo)   | 16K | 4096 | Updated pricing can be found on the [OpenAI pricing page](https://openai.com/pricing)\n",
        "| OpenAI  | [gpt-4-turbo-preview](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)  | 128K | 4096 | Updated pricing can be found on the [OpenAI pricing page](https://openai.com/pricing)\n",
        "| Google  | [gemini-pro](https://ai.google.dev/models/gemini?hl=en)    | 32760  | 2048 | Free access to Gemini pro through Google AI Studio with up to **60** requests per minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "F6Y4RJWe4vVx",
      "metadata": {
        "id": "F6Y4RJWe4vVx"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "cohere_api_key = userdata.get('cohere_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "db1a4ded-aec0-43b1-90a8-9796f7d32a44",
      "metadata": {
        "id": "db1a4ded-aec0-43b1-90a8-9796f7d32a44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc385d5-10d7-4349-ac46-4d222604c40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens per document: [592]\n",
            "Sum of tokens (documents): 592\n"
          ]
        }
      ],
      "source": [
        "# !pip install --upgrade tiktoken\n",
        "import tiktoken\n",
        "def tiktoken_tokens(documents,model=\"gpt-3.5-turbo-0125\"):\n",
        "    \"\"\"Use tiktoken (tokeniser for OpenAI models) to return a list of token length per document.\"\"\"\n",
        "\n",
        "    encoding = tiktoken.encoding_for_model(model) # returns the encoding used by the model.\n",
        "    tokens_length = [len(encoding.encode(doc)) for doc in documents]\n",
        "\n",
        "    return tokens_length\n",
        "\n",
        "\n",
        "# Calculate the number of tokens in each document.\n",
        "documents_length = tiktoken_tokens([doc.page_content for doc in documents],model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "print(\"Number of tokens per document:\",documents_length)\n",
        "print(\"Sum of tokens (documents):\",sum(documents_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bfe58ef-6d77-4394-a268-22e868cdcdce",
      "metadata": {
        "id": "2bfe58ef-6d77-4394-a268-22e868cdcdce"
      },
      "source": [
        ">The data scientist resume (generated by Chat GPT) has only 592 tokens. It is possible to extract several sections at once.\n",
        ">\n",
        ">However, our application is designed to analyse any CV, so we will extract information section by section to avoid exceeding the output token limit."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40921a93-4c0e-4ba9-b863-ed49ba85027a",
      "metadata": {
        "id": "40921a93-4c0e-4ba9-b863-ed49ba85027a"
      },
      "source": [
        "##### <a class=\"anchor\" id=\"embeddings\">Embeddings</a>\n",
        "\n",
        "Embeddings are numerical representations of text data in a high-dimensional vector space. For instance, the size of the embeddings vector size for OpenAI's text-embedding-ada-002 model is 1536.\n",
        "\n",
        "To identify the most similar documents to a query, we can search for vectors with the highest similarity to the query's embeddings. Cosine similarity is commonly used to measure the similarity between two vectors.s.\n",
        "\n",
        "We will connect to the following API endpoints for embeddings:\n",
        "\n",
        "|  Provider   |  Model | Vector dimension | Price\n",
        "| -------- | ------- | ------- | -------\n",
        "| OpenAI  | [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings/embedding-models)    | 1536 | **$0.00010 / 1K tokens.**\n",
        "| Google  | [models/embedding-001](https://ai.google.dev/models/gemini?hl=en)    | 768 | **Rate limit:** 1500 requests per minute. used:\n",
        "| Cohere  | [embed-english-light-v3.0]    | 384 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1c7d7476-413a-4fa7-bdc3-20b38eecc7e6",
      "metadata": {
        "id": "1c7d7476-413a-4fa7-bdc3-20b38eecc7e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f74d6ce-ff55-447a-8a68-9528797043ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-7bd81027a7eb>:14: LangChainDeprecationWarning: The class `CohereEmbeddings` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereEmbeddings``.\n",
            "  embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\",\n"
          ]
        }
      ],
      "source": [
        "def select_embeddings_model(LLM_service=\"Cohere\"):\n",
        "    \"\"\"Connect to the embeddings API endpoint by specifying the name of the embedding model.\"\"\"\n",
        "    if LLM_service == \"OpenAI\":\n",
        "        embeddings = OpenAIEmbeddings(\n",
        "            model='text-embedding-ada-002',\n",
        "            api_key=openai_api_key)\n",
        "\n",
        "    if LLM_service == \"Google\":\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(\n",
        "            model=\"models/embedding-001\",\n",
        "            google_api_key=google_api_key\n",
        "        )\n",
        "    if LLM_service == \"Cohere\":\n",
        "        embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\",\n",
        "            cohere_api_key=cohere_api_key,\n",
        "            user_agent=\"my-app/1.0\"  # Replace with your app's name/version\n",
        "                                      )\n",
        "    return embeddings\n",
        "\n",
        "# embeddings_OpenAI = select_embeddings_model(LLM_service=\"OpenAI\")\n",
        "# embeddings_google = select_embeddings_model(LLM_service=\"Google\")\n",
        "embeddings_cohere = select_embeddings_model(LLM_service=\"Cohere\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "h8kSyhUMfwdq",
      "metadata": {
        "id": "h8kSyhUMfwdq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "40c2f1fe-625f-470b-9cd4-d4904100da39",
      "metadata": {
        "id": "40c2f1fe-625f-470b-9cd4-d4904100da39"
      },
      "source": [
        "## <a class=\"anchor\" id=\"vectorstores\">Vectorstores</a>\n",
        "\n",
        "\n",
        "A vectorstore is a database used to store embedding vectors.\n",
        "\n",
        "There are several open-source options for vector storage. We will use the [Facebook AI Similarity Search (Faiss)](https://python.langchain.com/docs/integrations/vectorstores/faiss) vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "43a4d0f4-d412-42d7-88ec-4aee35ec5a6b",
      "metadata": {
        "id": "43a4d0f4-d412-42d7-88ec-4aee35ec5a6b"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "def create_vectorstore(embeddings, documents):\n",
        "    \"\"\"Create a Faiss vector database.\"\"\"\n",
        "    vector_store = FAISS.from_documents(documents=documents, embedding=embeddings)\n",
        "\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "275b70bf-3d07-4fb7-9e95-3feb804d87c9",
      "metadata": {
        "id": "275b70bf-3d07-4fb7-9e95-3feb804d87c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b61112-51de-4a97-ea12-bf68ae46c614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 30.2 ms, sys: 6.02 ms, total: 36.2 ms\n",
            "Wall time: 247 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "vector_store_Cohere = create_vectorstore(embeddings=embeddings_cohere,documents=documents)\n",
        "\n",
        "# vector_store_OpenAI = create_vectorstore(embeddings=embeddings_OpenAI,documents=documents)\n",
        "# vector_store_google = create_vectorstore(embeddings=embeddings_google,documents=documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34a172fd-9742-425e-a257-d29033abb28b",
      "metadata": {
        "id": "34a172fd-9742-425e-a257-d29033abb28b"
      },
      "source": [
        "## <a class=\"anchor\" id=\"retriever\">Vectorstore-backed retriever</a>\n",
        "\n",
        "\n",
        "A [retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/) is responsible for returning relevant documents to a query.\n",
        "\n",
        "**Vectorstore-backed retriever** is a simple retriever. It uses semantic search to retrieve documents from a Vectorstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4d61befc-c00c-4643-a958-c0c967f6190d",
      "metadata": {
        "id": "4d61befc-c00c-4643-a958-c0c967f6190d"
      },
      "outputs": [],
      "source": [
        "def Vectorstore_backed_retriever(vectorstore,search_type=\"similarity\",k=4,score_threshold=None):\n",
        "    \"\"\"create a vectorsore-backed retriever\n",
        "    Parameters:\n",
        "        search_type: Defines the type of search that the Retriever should perform.\n",
        "            Can be \"similarity\" (default), \"mmr\", or \"similarity_score_threshold\"\n",
        "        k: number of documents to return (Default: 4)\n",
        "        score_threshold: Minimum relevance threshold for similarity_score_threshold (default=None)\n",
        "    \"\"\"\n",
        "    search_kwargs={}\n",
        "    if k is not None:\n",
        "        search_kwargs['k'] = k\n",
        "    if score_threshold is not None:\n",
        "        search_kwargs['score_threshold'] = score_threshold\n",
        "\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=search_type,\n",
        "        search_kwargs=search_kwargs\n",
        "    )\n",
        "    return retriever\n",
        "\n",
        "base_retriever_Cohere = Vectorstore_backed_retriever(vector_store_Cohere,\"similarity\",k=min(4,len(documents)))\n",
        "\n",
        "# base_retriever_OpenAI = Vectorstore_backed_retriever(vector_store_OpenAI,\"similarity\",k=min(4,len(documents)))\n",
        "# base_retriever_google = Vectorstore_backed_retriever(vector_store_google,\"similarity\",k=min(4,len(documents)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a479af9-3f3d-4421-989b-89883dd40fda",
      "metadata": {
        "id": "7a479af9-3f3d-4421-989b-89883dd40fda"
      },
      "source": [
        "The most similar documents are not necessarily the most relevant.\n",
        "\n",
        "In the next section, we will create a CohereRerank retriever which will return the most relevant documents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b065429-36fe-4702-a836-7219c179d5bf",
      "metadata": {
        "id": "1b065429-36fe-4702-a836-7219c179d5bf"
      },
      "source": [
        "## <a class=\"anchor\" id=\"cohere\">Cohere reranker</a>\n",
        "\n",
        "We will wrap our base retriever with a [ContextualCompressionRetriever](https://python.langchain.com/docs/integrations/retrievers/cohere-reranker) and use the [Cohere rerank endpoint](https://docs.cohere.com/docs/reranking) to reorder the results based on semantically **relevance** to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f773aaa8-7851-4e6e-bb8d-e2fe136dbe11",
      "metadata": {
        "id": "f773aaa8-7851-4e6e-bb8d-e2fe136dbe11"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "\n",
        "def CohereRerank_retriever(\n",
        "    base_retriever,\n",
        "    cohere_api_key,cohere_model=\"rerank-multilingual-v2.0\", top_n=2\n",
        "):\n",
        "    \"\"\"Build a ContextualCompressionRetriever using Cohere Rerank endpoint to reorder the results based on relevance.\n",
        "    Parameters:\n",
        "       base_retriever: a Vectorstore-backed retriever\n",
        "       cohere_api_key: the Cohere API key\n",
        "       cohere_model: The Cohere model can be either 'rerank-english-v2.0' or 'rerank-multilingual-v2.0', with the latter being the default.\n",
        "       top_n: top n results returned by Cohere rerank, default = 2.\n",
        "    \"\"\"\n",
        "\n",
        "    compressor = CohereRerank(\n",
        "        cohere_api_key=cohere_api_key,\n",
        "        model=cohere_model,\n",
        "        top_n=top_n\n",
        "    )\n",
        "\n",
        "    retriever_Cohere = ContextualCompressionRetriever(\n",
        "        base_compressor=compressor,\n",
        "        base_retriever=base_retriever\n",
        "    )\n",
        "\n",
        "    return retriever_Cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e5c4eebd-8e6b-41a1-bfb0-83466898372a",
      "metadata": {
        "id": "e5c4eebd-8e6b-41a1-bfb0-83466898372a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc23fcf-8752-44e4-c374-1957acfda861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-163e53c3911b>:17: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereRerank``.\n",
            "  compressor = CohereRerank(\n"
          ]
        }
      ],
      "source": [
        "# retriever_Cohere_google = CohereRerank_retriever(\n",
        "#     base_retriever=base_retriever_google,\n",
        "#     cohere_api_key=cohere_api_key,\n",
        "#     cohere_model=\"rerank-multilingual-v2.0\",\n",
        "#     top_n=2\n",
        "# )\n",
        "\n",
        "# retriever_Cohere_openAI = CohereRerank_retriever(\n",
        "#     base_retriever=base_retriever_OpenAI,\n",
        "#     cohere_api_key=cohere_api_key,\n",
        "#     cohere_model=\"rerank-multilingual-v2.0\",\n",
        "#     top_n=2\n",
        "# )\n",
        "\n",
        "\n",
        "retriever_Cohere_Cohere = CohereRerank_retriever(\n",
        "    base_retriever=base_retriever_Cohere,\n",
        "    cohere_api_key=cohere_api_key,\n",
        "    cohere_model=\"rerank-multilingual-v2.0\",\n",
        "    top_n=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "80054b03-9d0a-42c6-9207-f6e2efd95486",
      "metadata": {
        "id": "80054b03-9d0a-42c6-9207-f6e2efd95486",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e2b28d-d828-4897-f6c0-d216a8d75695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-b7280ffba355>:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  most_relevant_docs = retriever_Cohere_Cohere.get_relevant_documents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar doc id : 0 \n"
          ]
        }
      ],
      "source": [
        "query = \"Extract the job title and company name of the first work experience.\"\n",
        "\n",
        "most_relevant_docs = retriever_Cohere_Cohere.get_relevant_documents(query)\n",
        "\n",
        "for i in range(len(most_relevant_docs)):\n",
        "    print(f\"\"\"Most similar doc id : {most_relevant_docs[i].metadata['doc_number']} \"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1319d81-c0ec-4ba1-a110-35d11a0669d5",
      "metadata": {
        "id": "b1319d81-c0ec-4ba1-a110-35d11a0669d5"
      },
      "source": [
        "# <a class=\"anchor\" id=\"llm\">Instantiate LLMs</a>\n",
        "\n",
        "Our appliaction will interact with LLMs, such as GPT3.5-turbo and Google gemini-pro.\n",
        "\n",
        "We will use the OpenAI and Google APIs, and leverage the `ChatOpenAI` and `ChatGoogleGenerativeAI` Langchain classes to instantiate the following instruction-tuned models:\n",
        "- **OpenAI API**: [gpt-3.5-turbo-0125](https://platform.openai.com/docs/models/gpt-3-5-turbo) and [gpt-4-turbo-preview](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo). \\\n",
        "These models are paid and prices are per 1,000 tokens (~750 words). Updated pricing can be found on the [OpenAI pricing page](https://openai.com/pricing).\n",
        "- **Google Gemini API**: [gemini-pro](https://ai.google.dev/models/gemini?hl=en). \\\n",
        "Free access to Gemini pro through Google AI Studio with up to 60 requests per minute.\n",
        "\n",
        "\n",
        "|  Provider   |  Model | maximum token limit | Output token limit | Price\n",
        "| -------- | ------- | ------- | ------- | -------\n",
        "| OpenAI  | [gpt-3.5-turbo-0125](https://platform.openai.com/docs/models/gpt-3-5-turbo)   | 16K | 4096 | Updated pricing can be found on the [OpenAI pricing page](https://openai.com/pricing)\n",
        "| OpenAI  | [gpt-4-turbo-preview](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)  | 128K | 4096 | Updated pricing can be found on the [OpenAI pricing page](https://openai.com/pricing)\n",
        "| Google  | [gemini-pro](https://ai.google.dev/models/gemini?hl=en)    | 32760  | 2048 | Free access to Gemini pro through Google AI Studio with up to **60** requests per minute.\n",
        "\n",
        "The main parameters for all of these LLMs are:\n",
        "- **temperature:** controls the degree of randomness in token selection. Higher values increase diversity, and hence, creativity.\n",
        "- **top_k:** Selects the next token from the most probable **k** tokens by using temperature. Lower k focuses on more probable tokens.\n",
        "- **top_p:** The cumulative probability cutoff for token selection. Higher values increase diversity.\n",
        "\n",
        "Let's create a function to instantiate a chat model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9b8aef78-02f2-4af2-bf31-a7d5dadf020c",
      "metadata": {
        "id": "9b8aef78-02f2-4af2-bf31-a7d5dadf020c"
      },
      "outputs": [],
      "source": [
        "def instantiate_LLM(LLM_provider,api_key,temperature=0.5,top_p=0.95,model_name=None):\n",
        "    \"\"\"Instantiate LLM in Langchain.\n",
        "    Parameters:\n",
        "        LLM_provider (str): the LLM provider; in [\"OpenAI\",\"Google\",\"HuggingFace\"]\n",
        "        model_name (str): in [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0125\", \"gpt-4-turbo-preview\",\n",
        "            \"gemini-pro\", \"mistralai/Mistral-7B-Instruct-v0.2\"].\n",
        "        api_key (str): google_api_key or openai_api_key or huggingfacehub_api_token\n",
        "        temperature (float): Range: 0.0 - 1.0; default = 0.5\n",
        "        top_p (float): : Range: 0.0 - 1.0; default = 1.\n",
        "    \"\"\"\n",
        "    # if LLM_provider == \"OpenAI\":\n",
        "    #     llm = ChatOpenAI(\n",
        "    #         api_key=api_key,\n",
        "    #         model=model_name,\n",
        "    #         temperature=temperature,\n",
        "    #         model_kwargs={\n",
        "    #             \"top_p\": top_p\n",
        "    #         }\n",
        "    #     )\n",
        "    # if LLM_provider == \"Google\":\n",
        "    #     llm = ChatGoogleGenerativeAI(\n",
        "    #         google_api_key=api_key,\n",
        "    #         # model=\"gemini-pro\",\n",
        "    #         model=model_name,\n",
        "    #         temperature=temperature,\n",
        "    #         top_p=top_p,\n",
        "    #         convert_system_message_to_human=True\n",
        "    #     )\n",
        "    if LLM_provider == \"Cohere\":\n",
        "        llm = ChatCohere(\n",
        "            cohere_api_key=api_key,\n",
        "            # model=\"gemini-pro\",\n",
        "            model=\"command-r-plus-08-2024\",\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "    return llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "7c69ee7f",
      "metadata": {
        "id": "7c69ee7f"
      },
      "outputs": [],
      "source": [
        "def set_LLM_and_retriever(provider=\"Cohere\",model_name=\"command-r-plus-08-2024\",temperature=0.0,top_p=0.95):\n",
        "    if provider==\"Cohere\":\n",
        "      llm = instantiate_LLM(\n",
        "          \"Cohere\",\n",
        "          api_key=cohere_api_key,\n",
        "          temperature=temperature,\n",
        "          top_p=top_p,\n",
        "          model_name=model_name\n",
        "      )\n",
        "      retriever = retriever_Cohere_Cohere\n",
        "    elif provider==\"OpenAI\":\n",
        "        llm = instantiate_LLM(\n",
        "            \"OpenAI\",\n",
        "            api_key=openai_api_key,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            model_name=model_name\n",
        "        )\n",
        "        retriever = retriever_Cohere_openAI\n",
        "    else: # \"Google\"\n",
        "        llm = instantiate_LLM(\n",
        "            LLM_provider=\"Google\",\n",
        "            api_key=google_api_key,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            model_name=\"gemini-pro\"\n",
        "        )\n",
        "        retriever = retriever_Cohere_google\n",
        "\n",
        "    return llm,retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "89db1613",
      "metadata": {
        "id": "89db1613"
      },
      "outputs": [],
      "source": [
        "llm,retriever = set_LLM_and_retriever(provider=\"Cohere\",temperature=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "jECFh4p4NUpH",
      "metadata": {
        "id": "jECFh4p4NUpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6028ae-8973-4fd3-a41b-12482fca8608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "client=<cohere.client.Client object at 0x7cca23516a50> async_client=<cohere.client.AsyncClient object at 0x7cca2f300ad0> model='command-r-plus-08-2024' temperature=0.0 cohere_api_key=SecretStr('**********')\n"
          ]
        }
      ],
      "source": [
        "print(llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c1c1446-4e6d-4149-a0c2-e93861c80a04",
      "metadata": {
        "id": "9c1c1446-4e6d-4149-a0c2-e93861c80a04"
      },
      "source": [
        "# <a class=\"anchor\" id=\"scanner\">Resume Scanner</a>\n",
        "\n",
        "In the following sections, we will call the LLM to:\n",
        "- Extract information from the resume (such as contact information, skills, work experience...).\n",
        "- Evaluate the quality of each section and bullet point and return a score on a scale from 0 to 100.\n",
        "- Improve the text and make it more appealing to recruiters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5300ff97",
      "metadata": {
        "id": "5300ff97"
      },
      "source": [
        "## <a class=\"anchor\" id=\"prompt_templates\">Prompt Templates</a>\n",
        "\n",
        "We will use the `PromptTemplate` class from Langchain to generate prompts for the LLM.\n",
        "\n",
        "The PromptTemplate will be generated using the following `templates`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2c87611b-d2bf-4765-af2d-1466584ef59a",
      "metadata": {
        "id": "2c87611b-d2bf-4765-af2d-1466584ef59a"
      },
      "outputs": [],
      "source": [
        "#####################################################\n",
        "#                 Prompt Templates\n",
        "#####################################################\n",
        "\n",
        "templates = {}\n",
        "\n",
        "# 2.1 Contact information Section\n",
        "templates[\n",
        "    \"Contact__information\"\n",
        "] = \"\"\"Extract and evaluate the contact information. \\\n",
        "Output a dictionary with the following keys:\n",
        "- candidate__name\n",
        "- candidate__title\n",
        "- candidate__location\n",
        "- candidate__email\n",
        "- candidate__phone\n",
        "- candidate__social_media: Extract a list of all social media profiles, blogs or websites.\n",
        "- evaluation__ContactInfo: Evaluate in {language} the contact information.\n",
        "- score__ContactInfo: Rate the contact information by giving a score (integer) from 0 to 100.\n",
        "\"\"\"\n",
        "\n",
        "# 2.2. Summary Section\n",
        "templates[\n",
        "    \"CV__summary\"\n",
        "] = \"\"\"Extract the summary and/or objective section. This is a separate section of the resume. \\\n",
        "If the resume doed not contain a summary and/or objective section, then simply write \"unknown\".\"\"\"\n",
        "\n",
        "# 2.3. WORK Experience Section\n",
        "\n",
        "templates[\n",
        "    \"Work__experience\"\n",
        "] = \"\"\"Extract all work experiences. For each work experience:\n",
        "1. Extract the job title.\n",
        "2. Extract the company.\n",
        "3. Extract the start date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "4. Extract the end date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "5. Output a dictionary with the following keys: job__title, job__company, job__start_date, job__end_date.\n",
        "\n",
        "Format your response as a list of dictionaries.\n",
        "\"\"\"\n",
        "\n",
        "# 2.4. Projects Section\n",
        "templates[\n",
        "    \"CV__Projects\"\n",
        "] = \"\"\"Include any side projects outside the work experience.\n",
        "For each project:\n",
        "1. Extract the title of the project.\n",
        "2. Extract the start date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "3. Extract the end date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "4. Output a dictionary with the following keys: project__title, project__start_date, project__end_date.\n",
        "\n",
        "Format your response as a list of dictionaries.\n",
        "\"\"\"\n",
        "\n",
        "# 2.5. Education Section\n",
        "templates[\n",
        "    \"CV__Education\"\n",
        "] = \"\"\"Extract all educational background and academic achievements.\n",
        "For each education achievement:\n",
        "1. Extract the name of the college or the high school.\n",
        "2. Extract the earned degree. Honors and achievements are included.\n",
        "3. Extract the start date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "4. Extract the end date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "5. Output a dictionary with the following keys: edu__college, edu__degree, edu__start_date, edu__end_date.\n",
        "\n",
        "Format your response as a list of dictionaries.\n",
        "\"\"\"\n",
        "\n",
        "templates[\n",
        "    \"Education__evaluation\"\n",
        "] = \"\"\"Your task is to perform the following actions:\n",
        "1. Rate the quality of the Education section by giving an integer score from 0 to 100.\n",
        "2. Evaluate (in three sentences and in {language}) the quality of the Education section.\n",
        "3. Output a dictionary with the following keys: score__edu, evaluation__edu.\n",
        "\"\"\"\n",
        "\n",
        "# 2.6. Skills\n",
        "templates[\n",
        "    \"candidate__skills\"\n",
        "] = \"\"\"Extract the list of soft and hard skills from the skill section. Output a list.\n",
        "The skill section is a separate section.\n",
        "\"\"\"\n",
        "\n",
        "templates[\n",
        "    \"Skills__evaluation\"\n",
        "] = \"\"\"Your task is to perform the following actions:\n",
        "1. Rate the quality of the Skills section by giving an integer score from 0 to 100.\n",
        "2. Evaluate (in three sentences and in {language}) the quality of the Skills section.\n",
        "3. Output a dictionary with the following keys: score__skills, evaluation__skills.\n",
        "\"\"\"\n",
        "\n",
        "# 2.7. Languages\n",
        "templates[\n",
        "    \"CV__Languages\"\n",
        "] = \"\"\"Extract all the languages that the candidate can speak. For each language:\n",
        "1. Extract the language.\n",
        "2. Extract the fluency. If the fluency is not available, then simply write \"unknown\".\n",
        "3. Output a dictionary with the following keys: spoken__language, language__fluency.\n",
        "\n",
        "Format your response as a list of dictionaries.\n",
        "\"\"\"\n",
        "\n",
        "templates[\n",
        "    \"Languages__evaluation\"\n",
        "] = \"\"\" Your task is to perform the following actions:\n",
        "1. Rate the quality of the language section by giving an integer score from 0 to 100.\n",
        "2. Evaluate (in three sentences and in {language}) the quality of the language section.\n",
        "3. Output a dictionary with the following keys: score__language,evaluation__language.\n",
        "\"\"\"\n",
        "\n",
        "# 2.8. Certifications\n",
        "templates[\n",
        "    \"CV__Certifications\"\n",
        "] = \"\"\"Extraction of all certificates other than education background and academic achievements. \\\n",
        "For each certificate:\n",
        "1. Extract the title of the certification.\n",
        "2. Extract the name of the organization or institution that issues the certification.\n",
        "3. Extract the date of certification and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "4. Extract the certification expiry date and output it in the following format: \\\n",
        "YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
        "5. Extract any other information listed about the certification. if not found, then simply write \"unknown\".\n",
        "6. Output a dictionary with the following keys: certif__title, certif__organization, certif__date, certif__expiry_date, certif__details.\n",
        "\n",
        "Format your response as a list of dictionaries.\n",
        "\"\"\"\n",
        "\n",
        "templates[\n",
        "    \"Certif__evaluation\"\n",
        "] = \"\"\"Your task is to perform the following actions:\n",
        "1. Rate the certifications by giving an integer score from 0 to 100.\n",
        "2. Evaluate (in three sentences and in {language}) the certifications and the quality of the text.\n",
        "3. Format your response as a dictionary with the following keys: score__certif,evaluation__certif.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 3. PROMPTS\n",
        "\n",
        "PROMPT_IMPROVE_SUMMARY = \"\"\"Your are given a resume (delimited by <resume></resume>) \\\n",
        "and a summary (delimited by <summary></summary>).\n",
        "1. In {language}, evaluate the summary (format and content) .\n",
        "2. Rate the summary by giving an integer score from 0 to 100. \\\n",
        "If the summary is \"unknown\", the score is 0.\n",
        "3. In {language}, strengthen the summary. The summary should not exceed 5 sentences. \\\n",
        "If the summary is \"unknown\", generate a strong summary in {language} with no more than 5 sentences. \\\n",
        "Please include: years of experience, top skills and experiences, some of the biggest achievements, and finally an attractive objective.\n",
        "4. Format your response as a dictionary with the following keys: evaluation__summary, score__summary, CV__summary_enhanced.\n",
        "\n",
        "<summary>\n",
        "{summary}\n",
        "</summary>\n",
        "------\n",
        "<resume>\n",
        "{resume}\n",
        "</resume>\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_IMPROVE_WORK_EXPERIENCE = \"\"\"you are given a work experience text delimited by triple backticks.\n",
        "1. Rate the quality of the work experience text by giving an integer score from 0 to 100.\n",
        "2. Suggest in {language} how to make the work experience text better and stronger.\n",
        "3. Strengthen the work experience text to make it more appealing to a recruiter in {language}. \\\n",
        "Provide additional details on responsibilities and quantify results for each bullet point. \\\n",
        "Format your text as a string in {language}.\n",
        "4. Format your response as a dictionary with the following keys: \"Score__WorkExperience\", \"Comments__WorkExperience\" and \"Improvement__WorkExperience\".\n",
        "\n",
        "Work experience text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_IMPROVE_PROJECT = \"\"\"you are given a project text delimited by triple backticks.\n",
        "1. Rate the quality of the project text by giving an integer score from 0 to 100.\n",
        "2. Suggest in {language} how to make the project text better and stronger.\n",
        "3. Strengthen the project text to make it more appealing to a recruiter in {language}, \\\n",
        "including the problem, the approach taken, the tools used and quantifiable results. \\\n",
        "Format your text as a string in {language}.\n",
        "4. Format your response as a dictionary with the following keys: Score__project, Comments__project, Improvement__project.\n",
        "\n",
        "project text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_EVALUATE_RESUME = \"\"\"You are given a resume delimited by triple backticks.\n",
        "1. Provide an overview of the resume in {language}.\n",
        "2. Provide a comprehensive analysis of the three main strengths of the resume in {language}. \\\n",
        "Format the top 3 strengths as string containg three bullet points.\n",
        "3. Provide a comprehensive analysis of the three main weaknesses of the resume in {language}. \\\n",
        "Format the top 3 weaknesses as string containg three bullet points.\n",
        "4. Format your response as a dictionary with the following keys: resume_cv_overview, top_3_strengths, top_3_weaknesses.\n",
        "\n",
        "The strengths and weaknesses lie in the format, style and content of the resume.\n",
        "\n",
        "Resume: ```{text}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "qzm8NIa0PX64",
      "metadata": {
        "id": "qzm8NIa0PX64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7731da6-da7b-4bb4-8077-a17a55ebb59e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Contact__information': 'Extract and evaluate the contact information. Output a dictionary with the following keys:\\n- candidate__name\\n- candidate__title\\n- candidate__location\\n- candidate__email\\n- candidate__phone\\n- candidate__social_media: Extract a list of all social media profiles, blogs or websites.\\n- evaluation__ContactInfo: Evaluate in {language} the contact information.\\n- score__ContactInfo: Rate the contact information by giving a score (integer) from 0 to 100.\\n', 'CV__summary': 'Extract the summary and/or objective section. This is a separate section of the resume. If the resume doed not contain a summary and/or objective section, then simply write \"unknown\".', 'Work__experience': 'Extract all work experiences. For each work experience:\\n1. Extract the job title.\\n2. Extract the company.\\n3. Extract the start date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\\n4. Extract the end date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\\n5. Output a dictionary with the following keys: job__title, job__company, job__start_date, job__end_date.\\n\\nFormat your response as a list of dictionaries.\\n', 'CV__Projects': 'Include any side projects outside the work experience.\\nFor each project:\\n1. Extract the title of the project.\\n2. Extract the start date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\\n3. Extract the end date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\\n4. Output a dictionary with the following keys: project__title, project__start_date, project__end_date.\\n\\nFormat your response as a list of dictionaries.\\n', 'CV__Education': 'Extract all educational background and academic achievements.\\nFor each education achievement:\\n1. Extract the name of the college or the high school.\\n2. Extract the earned degree. Honors and achievements are included.\\n3. Extract the start date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\\n4. Extract the end date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\\n5. Output a dictionary with the following keys: edu__college, edu__degree, edu__start_date, edu__end_date.\\n\\nFormat your response as a list of dictionaries.\\n', 'Education__evaluation': 'Your task is to perform the following actions:\\n1. Rate the quality of the Education section by giving an integer score from 0 to 100.\\n2. Evaluate (in three sentences and in {language}) the quality of the Education section.\\n3. Output a dictionary with the following keys: score__edu, evaluation__edu.\\n', 'candidate__skills': 'Extract the list of soft and hard skills from the skill section. Output a list.\\nThe skill section is a separate section.\\n', 'Skills__evaluation': 'Your task is to perform the following actions:\\n1. Rate the quality of the Skills section by giving an integer score from 0 to 100.\\n2. Evaluate (in three sentences and in {language}) the quality of the Skills section.\\n3. Output a dictionary with the following keys: score__skills, evaluation__skills.\\n', 'CV__Languages': 'Extract all the languages that the candidate can speak. For each language:\\n1. Extract the language.\\n2. Extract the fluency. If the fluency is not available, then simply write \"unknown\".\\n3. Output a dictionary with the following keys: spoken__language, language__fluency.\\n\\nFormat your response as a list of dictionaries.\\n', 'Languages__evaluation': ' Your task is to perform the following actions:\\n1. Rate the quality of the language section by giving an integer score from 0 to 100.\\n2. Evaluate (in three sentences and in {language}) the quality of the language section.\\n3. Output a dictionary with the following keys: score__language,evaluation__language.\\n', 'CV__Certifications': 'Extraction of all certificates other than education background and academic achievements. For each certificate:\\n1. Extract the title of the certification.\\n2. Extract the name of the organization or institution that issues the certification.\\n3. Extract the date of certification and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\\n4. Extract the certification expiry date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\\n5. Extract any other information listed about the certification. if not found, then simply write \"unknown\".\\n6. Output a dictionary with the following keys: certif__title, certif__organization, certif__date, certif__expiry_date, certif__details.\\n\\nFormat your response as a list of dictionaries.\\n', 'Certif__evaluation': 'Your task is to perform the following actions:\\n1. Rate the certifications by giving an integer score from 0 to 100.\\n2. Evaluate (in three sentences and in {language}) the certifications and the quality of the text.\\n3. Format your response as a dictionary with the following keys: score__certif,evaluation__certif.\\n'}\n"
          ]
        }
      ],
      "source": [
        "print(templates)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e659f4b-edca-45c3-9ef2-ee7ce15c1da9",
      "metadata": {
        "id": "9e659f4b-edca-45c3-9ef2-ee7ce15c1da9"
      },
      "source": [
        ">As you can see, we have provided **clear and specific instructions** to guide the model towards the desired output. To obtain relevant results, we have also:\n",
        ">- **Used delimiters** (such as triple backticks) for clear indication of context.\n",
        ">- **Formatted the output** as a JSON structured object. We will use the `json.loads` command. If it fails, parsing the formatted output will be straightforward.\n",
        ">- **Specified the steps** for completing a task. For example, to evaluate the skills section, we asked the model to first evaluate this section in three sentences, then rate its quality by giving an integer score from 0 to 100, and finally output a Json dictionary.\n",
        ">\n",
        ">**It is evident that LLM can perform various tasks in a single call. For instance, our inference tasks involve extracting information such as skills and academic achievements, evaluating the text's quality, and rating it on a scale of 0 to 100. LLMs are powerful as they can provide relevant answers for multiple tasks with just one direct prompt, without any examples (i.e. zero-shot), unlike traditional machine learning workflows that require a separate model for each task.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee5f3d84-8a53-4c8c-b4e4-aecae5eac02b",
      "metadata": {
        "id": "ee5f3d84-8a53-4c8c-b4e4-aecae5eac02b"
      },
      "source": [
        "Now, let's create a function to create the PromptTemplate from the templates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8528ebaa-452c-486b-88ef-f23d7b147e5d",
      "metadata": {
        "id": "8528ebaa-452c-486b-88ef-f23d7b147e5d"
      },
      "outputs": [],
      "source": [
        "def create_prompt_template(resume_sections, language=ASSISTAN_LANGUAGE):\n",
        "    \"\"\"Create the promptTemplate for selected resume sections.\n",
        "    Parameters:\n",
        "       resume_sections (list): List of resume sections from which information will be extracted.\n",
        "       language (str): the language of the AI assistant.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the final template: Add the templates from the 'templates' dictionary where keys = resume_sections\n",
        "    template = f\"\"\"For the following resume delimited by triple backticks, output in {language} the following information:\\n\\n\"\"\"\n",
        "\n",
        "    for key in resume_sections:\n",
        "        template += key + \": \" + templates[key] + \"\\n---------\\n\\n\"\n",
        "\n",
        "    template += \"For any requested information, if it is not found, output 'unknown'.\\n\\n\"\n",
        "    template += (\n",
        "        \"\"\"Format the final output as a json dictionary with the following keys: (\"\"\"\n",
        "    )\n",
        "\n",
        "    for key in resume_sections:\n",
        "        template += \"\" + key + \", \"\n",
        "    template = template[:-2] + \")\"  # remove the last \", \"\n",
        "\n",
        "    template += \"\"\"\\n\\nResume: ```{text}```\"\"\"\n",
        "\n",
        "    # Create the PromptTemplate\n",
        "    prompt_template = PromptTemplate.from_template(template)\n",
        "\n",
        "    return prompt_template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8822c8ee",
      "metadata": {
        "id": "8822c8ee"
      },
      "source": [
        "The final template includes a placeholder for the context (i.e. retrieved documents) and the assistant language. It instructs the LLM to answer the question based solely on the provided context. The `PromptTemplate` uses this template to return a LLM prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "94d87b4e-06c1-4689-a9d3-36f27ad708fa",
      "metadata": {
        "id": "94d87b4e-06c1-4689-a9d3-36f27ad708fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2a16be-2776-4e5f-bbbe-f4ef3ae99898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Number of tokens** in the prompt_template (instructions with no context): 341 tokens.\n",
            "\n",
            "**LLM prompt example:**\n",
            "\n",
            "For the following resume delimited by triple backticks, output in english the following information:\n",
            "\n",
            "Contact__information: Extract and evaluate the contact information. Output a dictionary with the following keys:\n",
            "- candidate__name\n",
            "- candidate__title\n",
            "- candidate__location\n",
            "- candidate__email\n",
            "- candidate__phone\n",
            "- candidate__social_media: Extract a list of all social media profiles, blogs or websites.\n",
            "- evaluation__ContactInfo: Evaluate in english the contact information.\n",
            "- score__ContactInfo: Rate the contact information by giving a score (integer) from 0 to 100.\n",
            "\n",
            "---------\n",
            "\n",
            "CV__summary: Extract the summary and/or objective section. This is a separate section of the resume. If the resume doed not contain a summary and/or objective section, then simply write \"unknown\".\n",
            "---------\n",
            "\n",
            "Work__experience: Extract all work experiences. For each work experience:\n",
            "1. Extract the job title.\n",
            "2. Extract the company.\n",
            "3. Extract the start date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "4. Extract the end date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "5. Output a dictionary with the following keys: job__title, job__company, job__start_date, job__end_date.\n",
            "\n",
            "Format your response as a list of dictionaries.\n",
            "\n",
            "---------\n",
            "\n",
            "For any requested information, if it is not found, output 'unknown'.\n",
            "\n",
            "Format the final output as a json dictionary with the following keys: (Contact__information, CV__summary, Work__experience)\n",
            "\n",
            "Resume: ```...```\n"
          ]
        }
      ],
      "source": [
        "# Here is a example:\n",
        "\n",
        "prompt_template = create_prompt_template(\n",
        "    ['Contact__information','CV__summary','Work__experience'],\n",
        "    language=ASSISTAN_LANGUAGE\n",
        ")\n",
        "\n",
        "print(\"**Number of tokens** in the prompt_template (instructions with no context):\",sum(tiktoken_tokens([prompt_template.template])),\"tokens.\")\n",
        "\n",
        "print(\"\\n**LLM prompt example:**\\n\")\n",
        "# Format the PromptTemplate\n",
        "prompt = prompt_template.format_prompt(text=\"...\",language=ASSISTAN_LANGUAGE).text\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecadcfe9-6d3e-4325-8faf-77c0f99b2043",
      "metadata": {
        "id": "ecadcfe9-6d3e-4325-8faf-77c0f99b2043"
      },
      "source": [
        "## <a class=\"anchor\" id=\"invoke_llms\">Invoke LLMs</a>\n",
        "\n",
        "Here, we create a function to invoke the LLM and get the content of its response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "cfd5db49-0b40-407b-93ca-ed4edd5ab0d5",
      "metadata": {
        "id": "cfd5db49-0b40-407b-93ca-ed4edd5ab0d5"
      },
      "outputs": [],
      "source": [
        "def invoke_LLM(\n",
        "    llm,\n",
        "    documents,\n",
        "    resume_sections: list,\n",
        "    info_message=\"\",\n",
        "    language=ASSISTAN_LANGUAGE,\n",
        "):\n",
        "    \"\"\"Invoke LLM and get a response.\n",
        "    Parameters:\n",
        "     - llm: the LLM to call\n",
        "     - documents: the Langchain Documents.\n",
        "     - resume_sections (list): List of resume sections to be parsed.\n",
        "     - info_message (str): display an informational message.\n",
        "     - language (str): the assistant language.\n",
        "\n",
        "     Output:\n",
        "     - response_content (str): the content of the LLM response.\n",
        "     - response_tokens_count (int): count of response tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    now = (datetime.datetime.now()).strftime(\"%H:%M:%S\")\n",
        "    print(f\"**{now}** \\t{info_message}\")\n",
        "\n",
        "    # 1. Create the promptTemplate.\n",
        "    prompt_template = create_prompt_template(\n",
        "        resume_sections,\n",
        "        language=language,\n",
        "    )\n",
        "\n",
        "    # 2. Format the PromptTemplate\n",
        "    if language is not None:\n",
        "        prompt = prompt_template.format_prompt(text=documents, language=language).text\n",
        "    else:\n",
        "        prompt = prompt_template.format_prompt(text=documents).text\n",
        "    print(\"prompt,\",prompt)\n",
        "\n",
        "    # 3. Invoke the LLM\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    response_content = response.content[\n",
        "        response.content.find(\"{\") : response.content.rfind(\"}\") + 1\n",
        "    ]\n",
        "    response_tokens_count = sum(tiktoken_tokens([response_content]))\n",
        "    print(f\"\"\"response tokens: {response_tokens_count}\"\"\")\n",
        "\n",
        "    return response_content, response_tokens_count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6568d60d-dec2-41de-a332-04d1fba40293",
      "metadata": {
        "id": "6568d60d-dec2-41de-a332-04d1fba40293"
      },
      "source": [
        "## <a class=\"anchor\" id=\"contact_info\">Contact Information</a>\n",
        "\n",
        "Name, Title, Location, Email, Phone number and Social media profiles will be extracted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "93aac393-3cb6-4bf0-98d5-fb8a9052e4b6",
      "metadata": {
        "id": "93aac393-3cb6-4bf0-98d5-fb8a9052e4b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461cab3c-fd6f-4eda-9b99-46f427a26a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**15:19:29** \tExtract and evaluate contact information...\n",
            "prompt, For the following resume delimited by triple backticks, output in english the following information:\n",
            "\n",
            "Contact__information: Extract and evaluate the contact information. Output a dictionary with the following keys:\n",
            "- candidate__name\n",
            "- candidate__title\n",
            "- candidate__location\n",
            "- candidate__email\n",
            "- candidate__phone\n",
            "- candidate__social_media: Extract a list of all social media profiles, blogs or websites.\n",
            "- evaluation__ContactInfo: Evaluate in english the contact information.\n",
            "- score__ContactInfo: Rate the contact information by giving a score (integer) from 0 to 100.\n",
            "\n",
            "---------\n",
            "\n",
            "For any requested information, if it is not found, output 'unknown'.\n",
            "\n",
            "Format the final output as a json dictionary with the following keys: (Contact__information)\n",
            "\n",
            "Resume: ```[Document(metadata={'source': '/content/data/resume/ChatGPT_dataScientist.pdf', 'doc_number': 0}, page_content='Name Candidate \\nAddress AAA \\ntest@gmail.com \\n123456789 \\n\\nObjective: \\nHighly skilled and experienced data scientist with a track record of success in both Company \\nX and Company Y. Proficient in a range of programming languages and tools, with a strong \\nability to lead teams and adapt to new challenges. Seeking opportunities to leverage \\nexpertise in data science to drive innovation and business growth. \\n\\nExperience: \\nData Scientist - Company Y (2019 - 2024) \\n\\n-  Led a team of data scientists in developing machine learning models for predictive \\n\\nanalytics. \\n\\n-  Utilized TensorFlow and PyTorch for deep learning projects, achieving significant \\n\\n- \\n\\nimprovements in model accuracy. \\nImplemented Spark and Spark MLlib for big data processing, optimizing performance \\nand scalability. \\n\\n-  Conducted data analysis using Tableau and Plotly, generating actionable insights for \\n\\nbusiness stakeholders. \\n\\n-  Collaborated with cross-functional teams to deploy machine learning solutions into \\n\\nproduction environments. \\n\\n-  Mentored junior team members, fostering their professional growth and development. \\n\\nData Scientist - Company X (2014 - 2019) \\n\\n-  Spearheaded data science initiatives, focusing on improving customer engagement \\n\\nand retention. \\n\\n-  Employed Python, Pandas, and Scikit-learn for data preprocessing and predictive \\n\\nmodeling tasks. \\n\\n-  Managed MySQL and Oracle databases, ensuring data integrity and accessibility for \\n\\nanalysis. \\n\\n-  Developed web applications using Flask and HTML to visualize and interact with \\n\\ndata. \\n\\n-  Served as a team leader, coordinating project timelines and priorities to meet \\n\\nbusiness objectives. \\n\\n-  Demonstrated adaptability in rapidly evolving business environments, delivering \\n\\nsolutions on time and within budget. \\n\\nEducation: \\nMaster of Science in Data Science \\nUniversity U1 \\n2014 \\n\\nSkills: \\nProgramming languages: Python, Java \\nData science: TensorFlow, PyTorch, Spark, Spark MLlib, Pandas, Scikit-learn \\nData analysis: Tableau, Plotly \\n\\n \\n \\n \\n \\n \\n\\x0cDatabases: MySQL, Oracle \\nWeb: Flask, HTML \\nSoft skills: Team leadership, Adaptability \\n\\nCertifications: \\nTensorFlow Developer Certificate (2022, Expires: 2025) \\n\\nRecent Projects: \\nChat with Your Data Using Retrieval Augmented Generation \\n\\n- \\n\\nImplemented a conversational AI system using retrieval augmented generation \\ntechniques. \\n\\n-  Leveraged natural language processing models to enable users to interact with data \\n\\nthrough chat interfaces. \\n\\n-  Enhanced user experience and accessibility of data-driven insights. \\n\\nImprove Resume Using the Power of LLM \\n\\n-  Applied large language model (LLM) techniques to optimize and personalize resume \\n\\ncontent. \\n\\n-  Utilized advanced natural language processing algorithms to highlight key skills and \\n\\nachievements effectively. \\n\\n- \\n\\nLanguages: \\nFluent in French, English, and Spanish.')]```\n",
            "response tokens: 137\n",
            "CPU times: user 21.4 ms, sys: 3.46 ms, total: 24.8 ms\n",
            "Wall time: 2.66 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Contact__information': {'candidate__name': 'Name Candidate',\n",
              "  'candidate__title': 'unknown',\n",
              "  'candidate__location': 'AAA',\n",
              "  'candidate__email': 'test@gmail.com',\n",
              "  'candidate__phone': '123456789',\n",
              "  'candidate__social_media': [],\n",
              "  'evaluation__ContactInfo': \"The resume provides basic contact information, including a name, address, email, and phone number. However, it lacks a professional title and any social media or online portfolio links, which could be valuable additions to showcase the candidate's expertise and online presence.\",\n",
              "  'score__ContactInfo': 70}}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "##########################################################################################################\n",
        "#     CONTACT_INFORMATION: Name, Title, Location, Email, Phone number and Social media profiles.\n",
        "##########################################################################################################\n",
        "\n",
        "try:\n",
        "    response_content, response_tokens_count = invoke_LLM(\n",
        "        llm,\n",
        "        documents,\n",
        "        resume_sections=[\"Contact__information\"],\n",
        "        info_message=\"Extract and evaluate contact information...\",\n",
        "        language=ASSISTAN_LANGUAGE,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Load response_content into json dictionary\n",
        "        CONTACT_INFORMATION = json.loads(response_content, strict=False)\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] json.loads returns error:\", e)\n",
        "        CONTACT_INFORMATION = {}\n",
        "\n",
        "except Exception as error:\n",
        "    print(\"[ERROR]:\", error)\n",
        "    CONTACT_INFORMATION = {}\n",
        "\n",
        "CONTACT_INFORMATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d1b6414-3dc1-4216-8d0b-924cfb2992d1",
      "metadata": {
        "id": "7d1b6414-3dc1-4216-8d0b-924cfb2992d1"
      },
      "source": [
        "**We will create a function to parse the response if json.loads fails.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "3413907c",
      "metadata": {
        "id": "3413907c"
      },
      "outputs": [],
      "source": [
        "def extract_from_text(text,start_tag,end_tag=None):\n",
        "    \"\"\"Use start and end tags to extract a substring from text.\"\"\"\n",
        "    start_index = text.find(start_tag)\n",
        "    if end_tag is None:\n",
        "        extacted_txt = text[start_index+len(start_tag):]\n",
        "    else:\n",
        "        end_index = text.find(end_tag)\n",
        "        extacted_txt = text[start_index+len(start_tag):end_index]\n",
        "\n",
        "    return extacted_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "67a87e1e",
      "metadata": {
        "id": "67a87e1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d8a75f0-440d-4124-812f-0ad0afdf3b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ERROR] json.loads returns error: Extra data: line 12 column 2 (char 601)\n",
            "\n",
            "['INFO'] Parse response content...\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Contact__information': {'candidate__name': 'Name Candidate',\n",
              "  'candidate__title': 'unknown',\n",
              "  'candidate__location': 'AAA',\n",
              "  'candidate__email': 'test@gmail.com',\n",
              "  'candidate__phone': '123456789',\n",
              "  'candidate__social_media': '],',\n",
              "  'evaluation__ContactInfo': 'The resume provides basic contact information, including a name, address, email, and phone number. However, it lacks a professional title and any social media or online portfolio links, which could be valuable additions to showcase the candidate\\'s expertise and online presence.\",',\n",
              "  'score__ContactInfo': 70}}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "try:\n",
        "    CONTACT_INFORMATION = json.loads(response_content+\"}\", strict=False) # Add \"}\" for error simulation\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] json.loads returns error:\",e)\n",
        "\n",
        "    # Parse the response_content\n",
        "    print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "    name = extract_from_text(response_content,\"\\\"candidate__name\\\": \",\"\\\"candidate__title\\\":\")\n",
        "    role = extract_from_text(response_content,\"\\\"candidate__title\\\": \",\"\\\"candidate__location\\\":\")\n",
        "    address = extract_from_text(response_content,\"\\\"candidate__location\\\": \",\"\\\"candidate__email\\\":\")\n",
        "    email_address = extract_from_text(response_content,\"\\\"candidate__email\\\": \",\"\\\"candidate__phone\\\":\")\n",
        "    phone_number = extract_from_text(response_content,\"\\\"candidate__phone\\\": \",\"\\\"candidate__social_media\\\":\")\n",
        "    social_media = extract_from_text(response_content,\"\\\"candidate__social_media\\\": \",\"\\\"evaluation__ContactInfo\\\":\")\n",
        "    eval__ContactInfo = extract_from_text(response_content,\"\\\"evaluation__ContactInfo\\\": \",\"\\\"score__ContactInfo\\\":\")\n",
        "    score__ContactInfo= extract_from_text(response_content,\"\\\"score__ContactInfo\\\": \",None)\n",
        "\n",
        "    # Create the dictionary\n",
        "    CONTACT_INFORMATION = {}\n",
        "\n",
        "    CONTACT_INFORMATION['Contact__information'] = {\n",
        "        'candidate__name': name[:name.rfind(\",\\n\")].strip()[1:-1],\n",
        "        'candidate__title': role[:role.rfind(\",\\n\")].strip()[1:-1],\n",
        "        'candidate__location': address[:address.rfind(\",\\n\")].strip()[1:-1],\n",
        "        'candidate__email': email_address[:email_address.rfind(\",\\n\")].strip()[1:-1],\n",
        "        'candidate__phone': phone_number[:phone_number.rfind(\",\\n\")].strip()[1:-1],\n",
        "        'candidate__social_media': social_media[0:social_media.rfind(\"}\\n\")-1][1:-1].strip(),\n",
        "        'evaluation__ContactInfo': eval__ContactInfo[0:eval__ContactInfo.rfind(\"}\\n\")-1][1:-1].strip(),\n",
        "        'score__ContactInfo': score__ContactInfo[0:score__ContactInfo.rfind(\"}\\n\")-1].strip()\n",
        "\n",
        "    }\n",
        "    # Convert score to int\n",
        "    try:\n",
        "        CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = int(\n",
        "            CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"]\n",
        "        )\n",
        "    except:\n",
        "        CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = -1\n",
        "\n",
        "CONTACT_INFORMATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43690e8f-b161-489b-906f-dc686fe10289",
      "metadata": {
        "id": "43690e8f-b161-489b-906f-dc686fe10289"
      },
      "source": [
        "**Let's create a function for parsing any response_content.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "f911ce39-8ccd-44c6-badd-3ba11dac79f0",
      "metadata": {
        "id": "f911ce39-8ccd-44c6-badd-3ba11dac79f0"
      },
      "outputs": [],
      "source": [
        "def ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car ):\n",
        "    \"\"\"This is a function to parse any response_content.\n",
        "    Parameters:\n",
        "     - response_content (str): the LLM response's content.\n",
        "     - list_fields (list): List of fields to parse.\n",
        "            A field can be a dictionary. The key of the dictionary will not be parsed.\n",
        "            Example: [{'Contact__information':['candidate__name','candidate__title','candidate__location']},\n",
        "                     'CV__summary']\n",
        "            The 'Contact__Information' field content will not be parsed in this example.\n",
        "     - list_rfind (list): To parse the content of a field, we first extract the text between this field and \\\n",
        "            the next field. Then, extract the text using the Python `rfind` command, which returns the highest index in the text \\\n",
        "            where the substring is found.\n",
        "     - list_exclude_first_car (list): Exclusion or not of the first and last characters (i.e. remove \\\")\n",
        "\n",
        "    Output:\n",
        "      - INFORMATION_dict: A dictionary, where fields are the keys and the extracted texts are the values.\n",
        "\n",
        "     \"\"\"\n",
        "    # 1. Format the list_fields as a list of tuples.\n",
        "    # Each tuple should contain the field, whether to extract information or not, and the key of the dictionary.\n",
        "    list_fields_formatted = []\n",
        "\n",
        "    for field in list_fields:\n",
        "        if type(field) is dict:\n",
        "            # The key of the dictionary will not be parsed.\n",
        "            list_fields_formatted.append((list(field.keys())[0],False,None))\n",
        "            for val in list(field.values())[0]:\n",
        "                list_fields_formatted.append((val,True,list(field.keys())[0]))\n",
        "        else:\n",
        "            list_fields_formatted.append((field,True,None))\n",
        "\n",
        "    list_fields_formatted.append((None,False,None))\n",
        "\n",
        "    # 2. Parse the response_content\n",
        "    Parsed_content = {}\n",
        "\n",
        "    for i in range(len(list_fields_formatted)-1):\n",
        "        if list_fields_formatted[i][1] is False:\n",
        "            Parsed_content[list_fields_formatted[i][0]] = {} # Initialize the dictionary\n",
        "        if list_fields_formatted[i][1]:\n",
        "            # 2.1. Extract the text between this field and the next one\n",
        "            extracted_value = extract_from_text(\n",
        "                response_content,\n",
        "                f\"\\\"{list_fields_formatted[i][0]}\\\": \",\n",
        "                f\"\\\"{list_fields_formatted[i+1][0]}\\\":\"\n",
        "            )\n",
        "\n",
        "            # 2.2. Extract the text using the Python `rfind` command, which returns the highest index in the text where the substring is found.\n",
        "            if list_rfind[i] is not None:\n",
        "                extracted_value = extracted_value[:extracted_value.rfind(list_rfind[i])].strip()\n",
        "\n",
        "            # 2.3. Remove the first and last characters (i.e. remove \\\")\n",
        "            if list_exclude_first_car[i]:\n",
        "                extracted_value = extracted_value[1:-1].strip()\n",
        "\n",
        "            # 2.4. Update the dictionary Parsed_content.\n",
        "            if list_fields_formatted[i][2] is None:\n",
        "                Parsed_content[list_fields_formatted[i][0]] = extracted_value\n",
        "            else:\n",
        "                Parsed_content[list_fields_formatted[i][2]][list_fields_formatted[i][0]] = extracted_value\n",
        "\n",
        "    return Parsed_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "96bfbe28-9549-4286-b955-2a719c18793e",
      "metadata": {
        "id": "96bfbe28-9549-4286-b955-2a719c18793e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e2c8fdd-03d1-4e4b-b140-5ea34556cfb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Contact__information': {'candidate__name': 'Name Candidate',\n",
              "  'candidate__title': 'unknown',\n",
              "  'candidate__location': 'AAA',\n",
              "  'candidate__email': 'test@gmail.com',\n",
              "  'candidate__phone': '123456789',\n",
              "  'candidate__social_media': '[]',\n",
              "  'evaluation__ContactInfo': \"The resume provides basic contact information, including a name, address, email, and phone number. However, it lacks a professional title and any social media or online portfolio links, which could be valuable additions to showcase the candidate's expertise and online presence.\",\n",
              "  'score__ContactInfo': 70}}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# List of keys. A field can be a dictionary. The key of the dict will not be parsed.\n",
        "list_fields = [{'Contact__information':['candidate__name','candidate__title','candidate__location',\n",
        "                                        'candidate__email','candidate__phone','candidate__social_media',\n",
        "                                        'evaluation__ContactInfo','score__ContactInfo']}]\n",
        "# list of rfind\n",
        "list_rfind = [\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\"}\\n\"]\n",
        "\n",
        "# Exclude last and first car:\n",
        "list_exclude_first_car = [True,True,True,True,True,True,False,True,False]\n",
        "\n",
        "# Parse response content\n",
        "CONTACT_INFORMATION = ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car)\n",
        "\n",
        "# Convert score to int\n",
        "try:\n",
        "    CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = int(\n",
        "        CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"]\n",
        "    )\n",
        "except:\n",
        "    CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = -1\n",
        "\n",
        "CONTACT_INFORMATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f565698d-ac80-44cd-9d43-252a81741bcb",
      "metadata": {
        "id": "f565698d-ac80-44cd-9d43-252a81741bcb"
      },
      "source": [
        "**Put it all together**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "d0900141-888a-45e2-87a0-0e924b732330",
      "metadata": {
        "id": "d0900141-888a-45e2-87a0-0e924b732330"
      },
      "outputs": [],
      "source": [
        "def Extract_contact_information(llm, documents):\n",
        "    \"\"\"Extract Contact Information: Name, Title, Location, Email, Phone number and Social media profiles.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response_content, response_tokens_count = invoke_LLM(\n",
        "            llm,\n",
        "            documents,\n",
        "            resume_sections=[\"Contact__information\"],\n",
        "            info_message=\"Extract and evaluate contact information...\",\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Load response_content into json dictionary\n",
        "            CONTACT_INFORMATION = json.loads(response_content, strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "            list_fields = [{'Contact__information':\n",
        "                            ['candidate__name','candidate__title','candidate__location',\n",
        "                             'candidate__email','candidate__phone','candidate__social_media',\n",
        "                             'evaluation__ContactInfo','score__ContactInfo']\n",
        "                           }]\n",
        "            list_rfind = [\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\"}\\n\"]\n",
        "            list_exclude_first_car = [True,True,True,True,True,True,False,True,False]\n",
        "            CONTACT_INFORMATION = ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car)\n",
        "            # Convert the score to int\n",
        "            try:\n",
        "                CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = int(\n",
        "                    CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"]\n",
        "                )\n",
        "            except:\n",
        "                CONTACT_INFORMATION[\"Contact__information\"][\"score__ContactInfo\"] = -1\n",
        "\n",
        "    except Exception as exception:\n",
        "        print(f\"[Error] {exception}\")\n",
        "        CONTACT_INFORMATION = {\n",
        "            \"Contact__information\": {\n",
        "                \"candidate__name\": \"unknown\",\n",
        "                \"candidate__title\": \"unknown\",\n",
        "                \"candidate__location\": \"unknown\",\n",
        "                \"candidate__email\": \"unknown\",\n",
        "                \"candidate__phone\": \"unknown\",\n",
        "                \"candidate__social_media\": \"unknown\",\n",
        "                \"evaluation__ContactInfo\": \"unknown\",\n",
        "                \"score__ContactInfo\": -1,\n",
        "            }\n",
        "        }\n",
        "\n",
        "    return CONTACT_INFORMATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "9aa7eb37-620b-4fca-b985-be3bf7f1eee6",
      "metadata": {
        "id": "9aa7eb37-620b-4fca-b985-be3bf7f1eee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2a053c-87a7-419c-ce3d-0ff173f9ae2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**15:19:34** \tExtract and evaluate contact information...\n",
            "prompt, For the following resume delimited by triple backticks, output in english the following information:\n",
            "\n",
            "Contact__information: Extract and evaluate the contact information. Output a dictionary with the following keys:\n",
            "- candidate__name\n",
            "- candidate__title\n",
            "- candidate__location\n",
            "- candidate__email\n",
            "- candidate__phone\n",
            "- candidate__social_media: Extract a list of all social media profiles, blogs or websites.\n",
            "- evaluation__ContactInfo: Evaluate in english the contact information.\n",
            "- score__ContactInfo: Rate the contact information by giving a score (integer) from 0 to 100.\n",
            "\n",
            "---------\n",
            "\n",
            "For any requested information, if it is not found, output 'unknown'.\n",
            "\n",
            "Format the final output as a json dictionary with the following keys: (Contact__information)\n",
            "\n",
            "Resume: ```[Document(metadata={'source': '/content/data/resume/ChatGPT_dataScientist.pdf', 'doc_number': 0}, page_content='Name Candidate \\nAddress AAA \\ntest@gmail.com \\n123456789 \\n\\nObjective: \\nHighly skilled and experienced data scientist with a track record of success in both Company \\nX and Company Y. Proficient in a range of programming languages and tools, with a strong \\nability to lead teams and adapt to new challenges. Seeking opportunities to leverage \\nexpertise in data science to drive innovation and business growth. \\n\\nExperience: \\nData Scientist - Company Y (2019 - 2024) \\n\\n-  Led a team of data scientists in developing machine learning models for predictive \\n\\nanalytics. \\n\\n-  Utilized TensorFlow and PyTorch for deep learning projects, achieving significant \\n\\n- \\n\\nimprovements in model accuracy. \\nImplemented Spark and Spark MLlib for big data processing, optimizing performance \\nand scalability. \\n\\n-  Conducted data analysis using Tableau and Plotly, generating actionable insights for \\n\\nbusiness stakeholders. \\n\\n-  Collaborated with cross-functional teams to deploy machine learning solutions into \\n\\nproduction environments. \\n\\n-  Mentored junior team members, fostering their professional growth and development. \\n\\nData Scientist - Company X (2014 - 2019) \\n\\n-  Spearheaded data science initiatives, focusing on improving customer engagement \\n\\nand retention. \\n\\n-  Employed Python, Pandas, and Scikit-learn for data preprocessing and predictive \\n\\nmodeling tasks. \\n\\n-  Managed MySQL and Oracle databases, ensuring data integrity and accessibility for \\n\\nanalysis. \\n\\n-  Developed web applications using Flask and HTML to visualize and interact with \\n\\ndata. \\n\\n-  Served as a team leader, coordinating project timelines and priorities to meet \\n\\nbusiness objectives. \\n\\n-  Demonstrated adaptability in rapidly evolving business environments, delivering \\n\\nsolutions on time and within budget. \\n\\nEducation: \\nMaster of Science in Data Science \\nUniversity U1 \\n2014 \\n\\nSkills: \\nProgramming languages: Python, Java \\nData science: TensorFlow, PyTorch, Spark, Spark MLlib, Pandas, Scikit-learn \\nData analysis: Tableau, Plotly \\n\\n \\n \\n \\n \\n \\n\\x0cDatabases: MySQL, Oracle \\nWeb: Flask, HTML \\nSoft skills: Team leadership, Adaptability \\n\\nCertifications: \\nTensorFlow Developer Certificate (2022, Expires: 2025) \\n\\nRecent Projects: \\nChat with Your Data Using Retrieval Augmented Generation \\n\\n- \\n\\nImplemented a conversational AI system using retrieval augmented generation \\ntechniques. \\n\\n-  Leveraged natural language processing models to enable users to interact with data \\n\\nthrough chat interfaces. \\n\\n-  Enhanced user experience and accessibility of data-driven insights. \\n\\nImprove Resume Using the Power of LLM \\n\\n-  Applied large language model (LLM) techniques to optimize and personalize resume \\n\\ncontent. \\n\\n-  Utilized advanced natural language processing algorithms to highlight key skills and \\n\\nachievements effectively. \\n\\n- \\n\\nLanguages: \\nFluent in French, English, and Spanish.')]```\n",
            "response tokens: 152\n",
            "CPU times: user 15.7 ms, sys: 3.95 ms, total: 19.6 ms\n",
            "Wall time: 2.71 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Contact__information': {'candidate__name': 'Name Candidate',\n",
              "  'candidate__title': 'unknown',\n",
              "  'candidate__location': 'AAA',\n",
              "  'candidate__email': 'test@gmail.com',\n",
              "  'candidate__phone': '123456789',\n",
              "  'candidate__social_media': 'unknown',\n",
              "  'evaluation__ContactInfo': \"The resume provides a clear and concise contact section with essential information. The candidate's name, email, and phone number are included, which is sufficient for initial contact purposes. However, the resume could be improved by adding a professional title and a more detailed address to provide a more comprehensive overview of the candidate's location.\",\n",
              "  'score__ContactInfo': 80}}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "CONTACT_INFORMATION = Extract_contact_information(llm,documents)\n",
        "CONTACT_INFORMATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1b9e141-52f3-4ead-8455-e9226b137b2c",
      "metadata": {
        "id": "d1b9e141-52f3-4ead-8455-e9226b137b2c"
      },
      "source": [
        "## <a class=\"anchor\" id=\"summary\">Summary</a>\n",
        "We will extract the summary section (if available), evaluate and strengthen it using LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "b3d775af-67f1-4009-b721-16f068d7ff3a",
      "metadata": {
        "id": "b3d775af-67f1-4009-b721-16f068d7ff3a"
      },
      "outputs": [],
      "source": [
        "def Extract_Evaluate_Summary(llm, documents):\n",
        "    \"\"\"Extract, evaluate and strengthen the summary.\"\"\"\n",
        "\n",
        "    # 1. Extract the summary\n",
        "    ######################################\n",
        "    try:\n",
        "        response_content, response_tokens_count = invoke_LLM(\n",
        "            llm,\n",
        "            documents,\n",
        "            resume_sections=[\"CV__summary\"],\n",
        "            info_message=\"Extract and evaluate the Summary....\",\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "        )\n",
        "        print(\"summary response:\\n\", response_content)\n",
        "        try:\n",
        "            # Load response_content into json dictionary\n",
        "            SUMMARY_SECTION = json.loads(response_content, strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "\n",
        "            list_fields = [\"CV__summary\"]\n",
        "            list_rfind = [\"}\\n\"]\n",
        "            list_exclude_first_car = [True]\n",
        "            SUMMARY_SECTION = ResponseContent_Parser(\n",
        "                response_content, list_fields, list_rfind, list_exclude_first_car\n",
        "            )\n",
        "\n",
        "    except Exception as exception:\n",
        "        print(f\"[Error] {exception}\")\n",
        "        SUMMARY_SECTION = {\"CV__summary\": \"unknown\"}\n",
        "\n",
        "    # 2. Evaluate and improve the summary\n",
        "    #############################################\n",
        "\n",
        "    try:\n",
        "        prompt_template = PromptTemplate.from_template(PROMPT_IMPROVE_SUMMARY)\n",
        "\n",
        "        prompt = prompt_template.format_prompt(\n",
        "            resume=documents,\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "            summary=SUMMARY_SECTION[\"CV__summary\"],\n",
        "        ).text\n",
        "\n",
        "        # Invoke LLM\n",
        "        response = llm.invoke(prompt)\n",
        "        response_content = response.content[\n",
        "            response.content.find(\"{\") : response.content.rfind(\"}\") + 1\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            SUMMARY_EVAL = {}\n",
        "            SUMMARY_EVAL[\"Summary__evaluation\"] = json.loads(response_content, strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "            list_fields = [\n",
        "                \"evaluation__summary\",\n",
        "                \"score__summary\",\n",
        "                \"CV__summary_enhanced\",\n",
        "            ]\n",
        "            list_rfind = [\",\\n\", \",\\n\", \"}\\n\"]\n",
        "            list_exclude_first_car = [True, False, True]\n",
        "            SUMMARY_EVAL[\"Summary__evaluation\"] = ResponseContent_Parser(\n",
        "                response_content, list_fields, list_rfind, list_exclude_first_car\n",
        "            )\n",
        "            # Convert score to int\n",
        "            try:\n",
        "                SUMMARY_EVAL[\"Summary__evaluation\"][\"score__summary\"] = int(\n",
        "                    SUMMARY_EVAL[\"Summary__evaluation\"][\"score__summary\"]\n",
        "                )\n",
        "            except:\n",
        "                SUMMARY_EVAL[\"Summary__evaluation\"][\"score__summary\"] = -1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        SUMMARY_EVAL = {\n",
        "            \"Summary__evaluation\": {\n",
        "                \"evaluation__summary\": \"unknown\",\n",
        "                \"score__summary\": -1,\n",
        "                \"CV__summary_enhanced\": \"unknown\",\n",
        "            }\n",
        "        }\n",
        "\n",
        "    SUMMARY_EVAL[\"CV__summary\"] = SUMMARY_SECTION[\"CV__summary\"]\n",
        "\n",
        "\n",
        "    return SUMMARY_EVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "d32064e0-885a-4382-9867-004f0bbfae0a",
      "metadata": {
        "id": "d32064e0-885a-4382-9867-004f0bbfae0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75599722-f2b3-4d9d-de0b-d5a7316700a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**15:19:38** \tExtract and evaluate the Summary....\n",
            "prompt, For the following resume delimited by triple backticks, output in english the following information:\n",
            "\n",
            "CV__summary: Extract the summary and/or objective section. This is a separate section of the resume. If the resume doed not contain a summary and/or objective section, then simply write \"unknown\".\n",
            "---------\n",
            "\n",
            "For any requested information, if it is not found, output 'unknown'.\n",
            "\n",
            "Format the final output as a json dictionary with the following keys: (CV__summary)\n",
            "\n",
            "Resume: ```[Document(metadata={'source': '/content/data/resume/ChatGPT_dataScientist.pdf', 'doc_number': 0}, page_content='Name Candidate \\nAddress AAA \\ntest@gmail.com \\n123456789 \\n\\nObjective: \\nHighly skilled and experienced data scientist with a track record of success in both Company \\nX and Company Y. Proficient in a range of programming languages and tools, with a strong \\nability to lead teams and adapt to new challenges. Seeking opportunities to leverage \\nexpertise in data science to drive innovation and business growth. \\n\\nExperience: \\nData Scientist - Company Y (2019 - 2024) \\n\\n-  Led a team of data scientists in developing machine learning models for predictive \\n\\nanalytics. \\n\\n-  Utilized TensorFlow and PyTorch for deep learning projects, achieving significant \\n\\n- \\n\\nimprovements in model accuracy. \\nImplemented Spark and Spark MLlib for big data processing, optimizing performance \\nand scalability. \\n\\n-  Conducted data analysis using Tableau and Plotly, generating actionable insights for \\n\\nbusiness stakeholders. \\n\\n-  Collaborated with cross-functional teams to deploy machine learning solutions into \\n\\nproduction environments. \\n\\n-  Mentored junior team members, fostering their professional growth and development. \\n\\nData Scientist - Company X (2014 - 2019) \\n\\n-  Spearheaded data science initiatives, focusing on improving customer engagement \\n\\nand retention. \\n\\n-  Employed Python, Pandas, and Scikit-learn for data preprocessing and predictive \\n\\nmodeling tasks. \\n\\n-  Managed MySQL and Oracle databases, ensuring data integrity and accessibility for \\n\\nanalysis. \\n\\n-  Developed web applications using Flask and HTML to visualize and interact with \\n\\ndata. \\n\\n-  Served as a team leader, coordinating project timelines and priorities to meet \\n\\nbusiness objectives. \\n\\n-  Demonstrated adaptability in rapidly evolving business environments, delivering \\n\\nsolutions on time and within budget. \\n\\nEducation: \\nMaster of Science in Data Science \\nUniversity U1 \\n2014 \\n\\nSkills: \\nProgramming languages: Python, Java \\nData science: TensorFlow, PyTorch, Spark, Spark MLlib, Pandas, Scikit-learn \\nData analysis: Tableau, Plotly \\n\\n \\n \\n \\n \\n \\n\\x0cDatabases: MySQL, Oracle \\nWeb: Flask, HTML \\nSoft skills: Team leadership, Adaptability \\n\\nCertifications: \\nTensorFlow Developer Certificate (2022, Expires: 2025) \\n\\nRecent Projects: \\nChat with Your Data Using Retrieval Augmented Generation \\n\\n- \\n\\nImplemented a conversational AI system using retrieval augmented generation \\ntechniques. \\n\\n-  Leveraged natural language processing models to enable users to interact with data \\n\\nthrough chat interfaces. \\n\\n-  Enhanced user experience and accessibility of data-driven insights. \\n\\nImprove Resume Using the Power of LLM \\n\\n-  Applied large language model (LLM) techniques to optimize and personalize resume \\n\\ncontent. \\n\\n-  Utilized advanced natural language processing algorithms to highlight key skills and \\n\\nachievements effectively. \\n\\n- \\n\\nLanguages: \\nFluent in French, English, and Spanish.')]```\n",
            "response tokens: 69\n",
            "summary response:\n",
            " {\n",
            "  \"CV__summary\": \"Highly skilled and experienced data scientist with a track record of success in both Company X and Company Y. Proficient in a range of programming languages and tools, with a strong ability to lead teams and adapt to new challenges. Seeking opportunities to leverage expertise in data science to drive innovation and business growth.\"\n",
            "}\n",
            "CPU times: user 33.5 ms, sys: 1.37 ms, total: 34.9 ms\n",
            "Wall time: 4.52 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Summary__evaluation': {'evaluation__summary': \"The summary is well-structured and provides a concise overview of the candidate's profile. It highlights the candidate's expertise in data science, programming languages, and leadership skills. The content is clear and to the point, covering the essential aspects of the candidate's experience and abilities.\",\n",
              "  'score__summary': 85,\n",
              "  'CV__summary_enhanced': \"A seasoned data scientist with 10+ years of experience, I have consistently delivered exceptional results at Company X and Company Y. My technical prowess spans programming languages (Python, Java), machine learning frameworks (TensorFlow, PyTorch), and data analysis tools (Tableau, Plotly). I have successfully led teams, mentored junior members, and adapted to dynamic environments. Notable achievements include developing conversational AI systems and optimizing resume content using LLMs. I aim to secure a position where I can leverage my expertise to drive innovation and contribute to the organization's success.\"},\n",
              " 'CV__summary': 'Highly skilled and experienced data scientist with a track record of success in both Company X and Company Y. Proficient in a range of programming languages and tools, with a strong ability to lead teams and adapt to new challenges. Seeking opportunities to leverage expertise in data science to drive innovation and business growth.'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "SUMMARY_EVAL = Extract_Evaluate_Summary(llm,documents)\n",
        "SUMMARY_EVAL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb4e3f7-1d2c-4978-abf2-1d6014807cf9",
      "metadata": {
        "id": "3eb4e3f7-1d2c-4978-abf2-1d6014807cf9"
      },
      "source": [
        "## <a class=\"anchor\" id=\"edu_n_language\">Education and Language</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "da9f5e47-da1f-42ea-a3ed-4a9fa7bea3a8",
      "metadata": {
        "id": "da9f5e47-da1f-42ea-a3ed-4a9fa7bea3a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4050764f-60f0-414b-b523-b745d402a4c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**15:55:31** \tExtract and evaluate education and language sections...\n",
            "prompt, For the following resume delimited by triple backticks, output in english the following information:\n",
            "\n",
            "CV__Education: Extract all educational background and academic achievements.\n",
            "For each education achievement:\n",
            "1. Extract the name of the college or the high school.\n",
            "2. Extract the earned degree. Honors and achievements are included.\n",
            "3. Extract the start date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "4. Extract the end date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "5. Output a dictionary with the following keys: edu__college, edu__degree, edu__start_date, edu__end_date.\n",
            "\n",
            "Format your response as a list of dictionaries.\n",
            "\n",
            "---------\n",
            "\n",
            "Education__evaluation: Your task is to perform the following actions:\n",
            "1. Rate the quality of the Education section by giving an integer score from 0 to 100.\n",
            "2. Evaluate (in three sentences and in english) the quality of the Education section.\n",
            "3. Output a dictionary with the following keys: score__edu, evaluation__edu.\n",
            "\n",
            "---------\n",
            "\n",
            "CV__Languages: Extract all the languages that the candidate can speak. For each language:\n",
            "1. Extract the language.\n",
            "2. Extract the fluency. If the fluency is not available, then simply write \"unknown\".\n",
            "3. Output a dictionary with the following keys: spoken__language, language__fluency.\n",
            "\n",
            "Format your response as a list of dictionaries.\n",
            "\n",
            "---------\n",
            "\n",
            "Languages__evaluation:  Your task is to perform the following actions:\n",
            "1. Rate the quality of the language section by giving an integer score from 0 to 100.\n",
            "2. Evaluate (in three sentences and in english) the quality of the language section.\n",
            "3. Output a dictionary with the following keys: score__language,evaluation__language.\n",
            "\n",
            "---------\n",
            "\n",
            "For any requested information, if it is not found, output 'unknown'.\n",
            "\n",
            "Format the final output as a json dictionary with the following keys: (CV__Education, Education__evaluation, CV__Languages, Languages__evaluation)\n",
            "\n",
            "Resume: ```[Document(metadata={'source': '/content/data/resume/ChatGPT_dataScientist.pdf', 'doc_number': 0}, page_content='Name Candidate \\nAddress AAA \\ntest@gmail.com \\n123456789 \\n\\nObjective: \\nHighly skilled and experienced data scientist with a track record of success in both Company \\nX and Company Y. Proficient in a range of programming languages and tools, with a strong \\nability to lead teams and adapt to new challenges. Seeking opportunities to leverage \\nexpertise in data science to drive innovation and business growth. \\n\\nExperience: \\nData Scientist - Company Y (2019 - 2024) \\n\\n-  Led a team of data scientists in developing machine learning models for predictive \\n\\nanalytics. \\n\\n-  Utilized TensorFlow and PyTorch for deep learning projects, achieving significant \\n\\n- \\n\\nimprovements in model accuracy. \\nImplemented Spark and Spark MLlib for big data processing, optimizing performance \\nand scalability. \\n\\n-  Conducted data analysis using Tableau and Plotly, generating actionable insights for \\n\\nbusiness stakeholders. \\n\\n-  Collaborated with cross-functional teams to deploy machine learning solutions into \\n\\nproduction environments. \\n\\n-  Mentored junior team members, fostering their professional growth and development. \\n\\nData Scientist - Company X (2014 - 2019) \\n\\n-  Spearheaded data science initiatives, focusing on improving customer engagement \\n\\nand retention. \\n\\n-  Employed Python, Pandas, and Scikit-learn for data preprocessing and predictive \\n\\nmodeling tasks. \\n\\n-  Managed MySQL and Oracle databases, ensuring data integrity and accessibility for \\n\\nanalysis. \\n\\n-  Developed web applications using Flask and HTML to visualize and interact with \\n\\ndata. \\n\\n-  Served as a team leader, coordinating project timelines and priorities to meet \\n\\nbusiness objectives. \\n\\n-  Demonstrated adaptability in rapidly evolving business environments, delivering \\n\\nsolutions on time and within budget. \\n\\nEducation: \\nMaster of Science in Data Science \\nUniversity U1 \\n2014 \\n\\nSkills: \\nProgramming languages: Python, Java \\nData science: TensorFlow, PyTorch, Spark, Spark MLlib, Pandas, Scikit-learn \\nData analysis: Tableau, Plotly \\n\\n \\n \\n \\n \\n \\n\\x0cDatabases: MySQL, Oracle \\nWeb: Flask, HTML \\nSoft skills: Team leadership, Adaptability \\n\\nCertifications: \\nTensorFlow Developer Certificate (2022, Expires: 2025) \\n\\nRecent Projects: \\nChat with Your Data Using Retrieval Augmented Generation \\n\\n- \\n\\nImplemented a conversational AI system using retrieval augmented generation \\ntechniques. \\n\\n-  Leveraged natural language processing models to enable users to interact with data \\n\\nthrough chat interfaces. \\n\\n-  Enhanced user experience and accessibility of data-driven insights. \\n\\nImprove Resume Using the Power of LLM \\n\\n-  Applied large language model (LLM) techniques to optimize and personalize resume \\n\\ncontent. \\n\\n-  Utilized advanced natural language processing algorithms to highlight key skills and \\n\\nachievements effectively. \\n\\n- \\n\\nLanguages: \\nFluent in French, English, and Spanish.')]```\n",
            "response tokens: 310\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CV__Education': [{'edu__college': 'University U1',\n",
              "   'edu__degree': 'Master of Science in Data Science',\n",
              "   'edu__start_date': '2014',\n",
              "   'edu__end_date': 'unknown'}],\n",
              " 'Education__evaluation': {'score__edu': 75,\n",
              "  'evaluation__edu': \"The Education section provides a clear overview of the candidate's academic background, highlighting a Master's degree in Data Science. However, it lacks detail on the specific courses or projects undertaken, which could provide further insight into the candidate's expertise. Including more information about the candidate's academic journey would enhance the overall quality of this section.\"},\n",
              " 'CV__Languages': [{'spoken__language': 'French',\n",
              "   'language__fluency': 'Fluent'},\n",
              "  {'spoken__language': 'English', 'language__fluency': 'Fluent'},\n",
              "  {'spoken__language': 'Spanish', 'language__fluency': 'Fluent'}],\n",
              " 'Languages__evaluation': {'score__language': 100,\n",
              "  'evaluation__language': \"The Languages section is comprehensive and effectively communicates the candidate's language skills. It lists three languages with fluency levels, providing a clear understanding of the candidate's multilingual abilities. This section is well-structured and easy to read, offering valuable information for potential employers.\"}}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "################################################################################\n",
        "#                        Education and Language\n",
        "################################################################################\n",
        "\n",
        "try:\n",
        "    response_content, response_tokens_count = invoke_LLM(\n",
        "        llm,\n",
        "        documents,\n",
        "        resume_sections=[\n",
        "            \"CV__Education\",\n",
        "            \"Education__evaluation\",\n",
        "            \"CV__Languages\",\n",
        "            \"Languages__evaluation\",\n",
        "        ],\n",
        "        info_message=\"Extract and evaluate education and language sections...\",\n",
        "        language=ASSISTAN_LANGUAGE,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Load response_content into json dictionary\n",
        "        Education_Language_sections = json.loads(response_content, strict=False)\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] json.loads returns error:\", e)\n",
        "        Education_Language_sections = {}\n",
        "\n",
        "except Exception as error:\n",
        "    print(\"[ERROR]:\", error)\n",
        "    Education_Language_sections = {}\n",
        "\n",
        "Education_Language_sections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24ac5bf6-4a4c-450a-aefe-452a6d716660",
      "metadata": {
        "id": "24ac5bf6-4a4c-450a-aefe-452a6d716660"
      },
      "source": [
        "**Parse response_content if json.loads fails**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "73f4f209-e435-4514-9ec6-15729c86e980",
      "metadata": {
        "id": "73f4f209-e435-4514-9ec6-15729c86e980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9589776-5d20-4931-c867-677590c1b422"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CV__Education': '{\\n      \"edu__college\": \"University U1\",\\n      \"edu__degree\": \"Master of Science in Data Science\",\\n      \"edu__start_date\": \"2014\",\\n      \"edu__end_date\": \"unknown\"\\n    }',\n",
              " 'Education__evaluation': {'score__edu': '75',\n",
              "  'evaluation__edu': 'The Education section provides a clear overview of the candidate\\'s academic background, highlighting a Master\\'s degree in Data Science. However, it lacks detail on the specific courses or projects undertaken, which could provide further insight into the candidate\\'s expertise. Including more information about the candidate\\'s academic journey would enhance the overall quality of this section.\"'},\n",
              " 'CV__Languages': '{\\n      \"spoken__language\": \"French\",\\n      \"language__fluency\": \"Fluent\"\\n    },\\n    {\\n      \"spoken__language\": \"English\",\\n      \"language__fluency\": \"Fluent\"\\n    },\\n    {\\n      \"spoken__language\": \"Spanish\",\\n      \"language__fluency\": \"Fluent\"\\n    }',\n",
              " 'Languages__evaluation': {'score__language': '100',\n",
              "  'evaluation__language': 'The Languages section is comprehensive and effectively communicates the candidate\\'s language skills. It lists three languages with fluency levels, providing a clear understanding of the candidate\\'s multilingual abilities. This section is well-structured and easy to read, offering valuable information for potential employers.\"'}}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "list_fields = ['CV__Education',\n",
        "               {'Education__evaluation':['score__edu','evaluation__edu']},\n",
        "               'CV__Languages',\n",
        "               {'Languages__evaluation':['score__language','evaluation__language']},\n",
        "              ]\n",
        "\n",
        "list_rfind = [\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\",\\n\",\"\\n\"]\n",
        "list_exclude_first_car = [True,True,False,True,True,True,False,True]\n",
        "\n",
        "Education_Language_sections = ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car)\n",
        "Education_Language_sections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bbd4e3c-6a0b-4087-be23-9177169a5959",
      "metadata": {
        "id": "0bbd4e3c-6a0b-4087-be23-9177169a5959"
      },
      "source": [
        "**Now we need to split the list of languages and educational background.**\n",
        "\n",
        "We will create a function to split a text containing a list of dicts into a Python list of dicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "2ab2df01-3d83-401d-b9f1-3ef30d9bc2dc",
      "metadata": {
        "id": "2ab2df01-3d83-401d-b9f1-3ef30d9bc2dc"
      },
      "outputs": [],
      "source": [
        "def convert_text_to_list_of_dicts(text,dict_keys):\n",
        "    \"\"\"Convert text to a python list of dicts.\n",
        "    Parameters:\n",
        "     - text: string containing a list of dicts\n",
        "     - dict_keys (list): the keys of the dictionary which will be returned.\n",
        "    Output:\n",
        "     - list_of_dicts (list): the list of dicts to return.\n",
        "     \"\"\"\n",
        "    list_of_dicts = []\n",
        "\n",
        "    if text!=\"\": # if non-empty list\n",
        "        text_splitted = text.split(\"},\\n\")\n",
        "        dict_keys.append(None)\n",
        "\n",
        "        for i in range(len(text_splitted)):\n",
        "            dict_i = {}\n",
        "\n",
        "            for j in range(len(dict_keys)-1):\n",
        "                key_value = extract_from_text(text_splitted[i],f\"\\\"{dict_keys[j]}\\\": \",f\"\\\"{dict_keys[j+1]}\\\": \")\n",
        "                key_value = key_value[:key_value.rfind(\",\\n\")].strip()[1:-1]\n",
        "                dict_i[dict_keys[j]] = key_value\n",
        "\n",
        "            list_of_dicts.append(dict_i) # add the dict to the list.\n",
        "\n",
        "    return list_of_dicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "dcb955b6-129d-4625-85f3-6f42371bc228",
      "metadata": {
        "id": "dcb955b6-129d-4625-85f3-6f42371bc228",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b81c9c-6f36-4df9-e44b-c4044d261ff5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'spoken__language': 'French', 'language__fluency': 'Fluent'},\n",
              " {'spoken__language': 'English', 'language__fluency': 'Fluent'},\n",
              " {'spoken__language': 'Spanish', 'language__fluency': 'Flue'}]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "languages = Education_Language_sections['CV__Languages']\n",
        "Education_Language_sections['CV__Languages'] = convert_text_to_list_of_dicts(\n",
        "    text = languages[languages.find('[')+1:languages.rfind(\"]\")].strip(),\n",
        "    dict_keys = ['spoken__language','language__fluency']\n",
        ")\n",
        "Education_Language_sections['CV__Languages']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "22c5f8dc-3496-48f0-93e6-00f657253c04",
      "metadata": {
        "id": "22c5f8dc-3496-48f0-93e6-00f657253c04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a846a9e-5260-4e84-d98e-cdd4b63fe3b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'edu__college': 'University U1',\n",
              "  'edu__degree': 'Master of Science in Data Science',\n",
              "  'edu__start_date': '2014',\n",
              "  'edu__end_date': 'unkno'}]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "education = Education_Language_sections['CV__Education']\n",
        "Education_Language_sections['CV__Education'] = convert_text_to_list_of_dicts(\n",
        "    text = education[education.find('[')+1:education.rfind(\"]\")].strip(),\n",
        "    dict_keys = ['edu__college','edu__degree','edu__start_date','edu__end_date']\n",
        ")\n",
        "Education_Language_sections['CV__Education']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b55e8f4-f463-4e25-a4e7-5fc2263b71ef",
      "metadata": {
        "id": "3b55e8f4-f463-4e25-a4e7-5fc2263b71ef"
      },
      "source": [
        "**Putting it all together**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "a7712aec-c76d-444e-8d1e-0e7961919e99",
      "metadata": {
        "id": "a7712aec-c76d-444e-8d1e-0e7961919e99"
      },
      "outputs": [],
      "source": [
        "def Extract_Education_Language(llm, documents):\n",
        "    \"\"\"Extract and evaluate education and language sections.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response_content, response_tokens_count = invoke_LLM(\n",
        "            llm,\n",
        "            documents,\n",
        "            resume_sections=[\n",
        "                \"CV__Education\",\n",
        "                \"Education__evaluation\",\n",
        "                \"CV__Languages\",\n",
        "                \"Languages__evaluation\",\n",
        "            ],\n",
        "            info_message=\"Extract and evaluate education and language sections...\",\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Load response_content into json dictionary\n",
        "            Education_Language_sections = json.loads(response_content, strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "\n",
        "            list_fields = [\n",
        "                \"CV__Education\",\n",
        "                {\"Education__evaluation\": [\"score__edu\", \"evaluation__edu\"]},\n",
        "                \"CV__Languages\",\n",
        "                {\"Languages__evaluation\": [\"score__language\", \"evaluation__language\"]},\n",
        "            ]\n",
        "\n",
        "            list_rfind = [\",\\n\", \",\\n\", \",\\n\", \",\\n\", \",\\n\", \",\\n\", \",\\n\", \"\\n\"]\n",
        "            list_exclude_first_car = [True, True, False, True, True, True, False, True]\n",
        "\n",
        "            Education_Language_sections = ResponseContent_Parser(response_content, list_fields, list_rfind, list_exclude_first_car)\n",
        "\n",
        "            # Convert scores to int\n",
        "            try:\n",
        "                Education_Language_sections[\"Education__evaluation\"][\"score__edu\"] = int(\n",
        "                    Education_Language_sections[\"Education__evaluation\"][\"score__edu\"]\n",
        "                )\n",
        "            except:\n",
        "                Education_Language_sections[\"Education__evaluation\"][\"score__edu\"] = -1\n",
        "\n",
        "            try:\n",
        "                Education_Language_sections[\"Languages__evaluation\"][\"score__language\"] = int(\n",
        "                    Education_Language_sections[\"Languages__evaluation\"][\"score__language\"]\n",
        "                )\n",
        "            except:\n",
        "                Education_Language_sections[\"Languages__evaluation\"][\"score__language\"] = -1\n",
        "\n",
        "            # Split languages and educational texts into a Python list of dict\n",
        "            languages = Education_Language_sections[\"CV__Languages\"]\n",
        "            Education_Language_sections[\"CV__Languages\"] = (\n",
        "                convert_text_to_list_of_dicts(\n",
        "                    text=languages[\n",
        "                        languages.find(\"[\") + 1 : languages.rfind(\"]\")\n",
        "                    ].strip(),\n",
        "                    dict_keys=[\"spoken__language\", \"language__fluency\"],\n",
        "                )\n",
        "            )\n",
        "            education = Education_Language_sections[\"CV__Education\"]\n",
        "            Education_Language_sections[\"CV__Education\"] = (\n",
        "                convert_text_to_list_of_dicts(\n",
        "                    text=education[\n",
        "                        education.find(\"[\") + 1 : education.rfind(\"]\")\n",
        "                    ].strip(),\n",
        "                    dict_keys=[\n",
        "                        \"edu__college\",\n",
        "                        \"edu__degree\",\n",
        "                        \"edu__start_date\",\n",
        "                        \"edu__end_date\",\n",
        "                    ],\n",
        "                )\n",
        "            )\n",
        "    except Exception as exception:\n",
        "        print(f\"[Error] {exception}\")\n",
        "        Education_Language_sections = {\n",
        "            \"CV__Education\": [],\n",
        "            \"Education__evaluation\": {\n",
        "                \"score__edu\": -1,\n",
        "                \"evaluation__edu\": \"unknown\"\n",
        "            },\n",
        "            \"CV__Languages\": [],\n",
        "            \"Languages__evaluation\": {\n",
        "                \"score__language\": -1,\n",
        "                \"evaluation__language\": \"unknown\",\n",
        "            },\n",
        "        }\n",
        "\n",
        "    return Education_Language_sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "5dc5c85d-371d-44b3-9d16-79ea7c97e75b",
      "metadata": {
        "id": "5dc5c85d-371d-44b3-9d16-79ea7c97e75b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c297a50b-126f-4072-df6c-d91df2e5be2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**15:55:36** \tExtract and evaluate education and language sections...\n",
            "prompt, For the following resume delimited by triple backticks, output in english the following information:\n",
            "\n",
            "CV__Education: Extract all educational background and academic achievements.\n",
            "For each education achievement:\n",
            "1. Extract the name of the college or the high school.\n",
            "2. Extract the earned degree. Honors and achievements are included.\n",
            "3. Extract the start date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "4. Extract the end date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "5. Output a dictionary with the following keys: edu__college, edu__degree, edu__start_date, edu__end_date.\n",
            "\n",
            "Format your response as a list of dictionaries.\n",
            "\n",
            "---------\n",
            "\n",
            "Education__evaluation: Your task is to perform the following actions:\n",
            "1. Rate the quality of the Education section by giving an integer score from 0 to 100.\n",
            "2. Evaluate (in three sentences and in english) the quality of the Education section.\n",
            "3. Output a dictionary with the following keys: score__edu, evaluation__edu.\n",
            "\n",
            "---------\n",
            "\n",
            "CV__Languages: Extract all the languages that the candidate can speak. For each language:\n",
            "1. Extract the language.\n",
            "2. Extract the fluency. If the fluency is not available, then simply write \"unknown\".\n",
            "3. Output a dictionary with the following keys: spoken__language, language__fluency.\n",
            "\n",
            "Format your response as a list of dictionaries.\n",
            "\n",
            "---------\n",
            "\n",
            "Languages__evaluation:  Your task is to perform the following actions:\n",
            "1. Rate the quality of the language section by giving an integer score from 0 to 100.\n",
            "2. Evaluate (in three sentences and in english) the quality of the language section.\n",
            "3. Output a dictionary with the following keys: score__language,evaluation__language.\n",
            "\n",
            "---------\n",
            "\n",
            "For any requested information, if it is not found, output 'unknown'.\n",
            "\n",
            "Format the final output as a json dictionary with the following keys: (CV__Education, Education__evaluation, CV__Languages, Languages__evaluation)\n",
            "\n",
            "Resume: ```[Document(metadata={'source': '/content/data/resume/ChatGPT_dataScientist.pdf', 'doc_number': 0}, page_content='Name Candidate \\nAddress AAA \\ntest@gmail.com \\n123456789 \\n\\nObjective: \\nHighly skilled and experienced data scientist with a track record of success in both Company \\nX and Company Y. Proficient in a range of programming languages and tools, with a strong \\nability to lead teams and adapt to new challenges. Seeking opportunities to leverage \\nexpertise in data science to drive innovation and business growth. \\n\\nExperience: \\nData Scientist - Company Y (2019 - 2024) \\n\\n-  Led a team of data scientists in developing machine learning models for predictive \\n\\nanalytics. \\n\\n-  Utilized TensorFlow and PyTorch for deep learning projects, achieving significant \\n\\n- \\n\\nimprovements in model accuracy. \\nImplemented Spark and Spark MLlib for big data processing, optimizing performance \\nand scalability. \\n\\n-  Conducted data analysis using Tableau and Plotly, generating actionable insights for \\n\\nbusiness stakeholders. \\n\\n-  Collaborated with cross-functional teams to deploy machine learning solutions into \\n\\nproduction environments. \\n\\n-  Mentored junior team members, fostering their professional growth and development. \\n\\nData Scientist - Company X (2014 - 2019) \\n\\n-  Spearheaded data science initiatives, focusing on improving customer engagement \\n\\nand retention. \\n\\n-  Employed Python, Pandas, and Scikit-learn for data preprocessing and predictive \\n\\nmodeling tasks. \\n\\n-  Managed MySQL and Oracle databases, ensuring data integrity and accessibility for \\n\\nanalysis. \\n\\n-  Developed web applications using Flask and HTML to visualize and interact with \\n\\ndata. \\n\\n-  Served as a team leader, coordinating project timelines and priorities to meet \\n\\nbusiness objectives. \\n\\n-  Demonstrated adaptability in rapidly evolving business environments, delivering \\n\\nsolutions on time and within budget. \\n\\nEducation: \\nMaster of Science in Data Science \\nUniversity U1 \\n2014 \\n\\nSkills: \\nProgramming languages: Python, Java \\nData science: TensorFlow, PyTorch, Spark, Spark MLlib, Pandas, Scikit-learn \\nData analysis: Tableau, Plotly \\n\\n \\n \\n \\n \\n \\n\\x0cDatabases: MySQL, Oracle \\nWeb: Flask, HTML \\nSoft skills: Team leadership, Adaptability \\n\\nCertifications: \\nTensorFlow Developer Certificate (2022, Expires: 2025) \\n\\nRecent Projects: \\nChat with Your Data Using Retrieval Augmented Generation \\n\\n- \\n\\nImplemented a conversational AI system using retrieval augmented generation \\ntechniques. \\n\\n-  Leveraged natural language processing models to enable users to interact with data \\n\\nthrough chat interfaces. \\n\\n-  Enhanced user experience and accessibility of data-driven insights. \\n\\nImprove Resume Using the Power of LLM \\n\\n-  Applied large language model (LLM) techniques to optimize and personalize resume \\n\\ncontent. \\n\\n-  Utilized advanced natural language processing algorithms to highlight key skills and \\n\\nachievements effectively. \\n\\n- \\n\\nLanguages: \\nFluent in French, English, and Spanish.')]```\n",
            "response tokens: 310\n",
            "CPU times: user 23.9 ms, sys: 4.31 ms, total: 28.2 ms\n",
            "Wall time: 5.08 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CV__Education': [{'edu__college': 'University U1',\n",
              "   'edu__degree': 'Master of Science in Data Science',\n",
              "   'edu__start_date': '2014',\n",
              "   'edu__end_date': 'unknown'}],\n",
              " 'Education__evaluation': {'score__edu': 75,\n",
              "  'evaluation__edu': \"The Education section provides a clear overview of the candidate's academic background, highlighting a Master's degree in Data Science. However, it lacks detail on the specific courses or projects undertaken, which could provide further insight into the candidate's expertise. Including more information about the candidate's academic journey would enhance the overall quality of this section.\"},\n",
              " 'CV__Languages': [{'spoken__language': 'French',\n",
              "   'language__fluency': 'Fluent'},\n",
              "  {'spoken__language': 'English', 'language__fluency': 'Fluent'},\n",
              "  {'spoken__language': 'Spanish', 'language__fluency': 'Fluent'}],\n",
              " 'Languages__evaluation': {'score__language': 100,\n",
              "  'evaluation__language': \"The Languages section is comprehensive and effectively communicates the candidate's language skills. It lists three languages with fluency levels, providing a clear understanding of the candidate's multilingual abilities. This section is well-structured and easy to read, offering valuable information for potential employers.\"}}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "Education_Language_sections = Extract_Education_Language(llm,documents)\n",
        "Education_Language_sections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba9d6e1-4677-4e75-a85d-f2b1a9cb29ba",
      "metadata": {
        "id": "aba9d6e1-4677-4e75-a85d-f2b1a9cb29ba"
      },
      "source": [
        "## <a class=\"anchor\" id=\"skills_certifs\">Skills and  Certifications</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "246f46e0-d670-4711-864e-05308fe9b698",
      "metadata": {
        "id": "246f46e0-d670-4711-864e-05308fe9b698"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "#                     SKILLS and Certifications\n",
        "################################################################################\n",
        "\n",
        "def Extract_Skills_and_Certifications(llm, documents):\n",
        "    \"\"\"Extract Skills and certifications and evaluate these sections.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response_content, response_tokens_count = invoke_LLM(\n",
        "            llm,\n",
        "            documents,\n",
        "            resume_sections=[\"candidate__skills\",\"Skills__evaluation\",\"CV__Certifications\",\"Certif__evaluation\"],\n",
        "            info_message=\"Extract and evaluate the skills and certifications...\",\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "        )\n",
        "        print(\"response_content, response_tokens_count\" ,response_content, response_tokens_count )\n",
        "        try:\n",
        "            # Load response_content into json dictionary\n",
        "            SKILLS_and_CERTIF = json.loads(response_content, strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "\n",
        "            skills = extract_from_text(response_content,\"\\\"candidate__skills\\\": \", \"\\\"Skills__evaluation\\\":\")\n",
        "            skills = skills.replace(\"\\n  \",\"\\n\").replace(\"],\\n\",\"\").replace(\"[\\n\",\"\")\n",
        "            score_skills = extract_from_text(response_content,\"\\\"score__skills\\\": \", \"\\\"evaluation__skills\\\":\")\n",
        "            evaluation_skills = extract_from_text(response_content,\"\\\"evaluation__skills\\\": \", \"\\\"CV__Certifications\\\":\")\n",
        "\n",
        "            certif_text = extract_from_text(response_content,\"\\\"CV__Certifications\\\": \", \"\\\"Certif__evaluation\\\":\")\n",
        "            certif_score = extract_from_text(response_content,\"\\\"score__certif\\\": \", \"\\\"evaluation__certif\\\":\")\n",
        "            certif_eval = extract_from_text(response_content,\"\\\"evaluation__certif\\\": \", None)\n",
        "\n",
        "\n",
        "            # Create the dictionary\n",
        "            SKILLS_and_CERTIF = {}\n",
        "            SKILLS_and_CERTIF[\"candidate__skills\"] = [skill.strip()[1:-1] for skill in skills.split(\",\\n\")]\n",
        "\n",
        "            # Convert the score to int\n",
        "            try:\n",
        "                score_skills_int = int(score_skills[0 : score_skills.rfind(\",\\n\")])\n",
        "            except:\n",
        "                score_skills_int = -1\n",
        "\n",
        "            SKILLS_and_CERTIF[\"Skills__evaluation\"] = {\n",
        "                \"score__skills\": score_skills_int,\n",
        "                \"evaluation__skills\": evaluation_skills[: evaluation_skills.rfind(\"}\\n\")].strip()[1:-1],\n",
        "            }\n",
        "\n",
        "            # Convert text to list of dictionaries\n",
        "            list_certifs = convert_text_to_list_of_dicts(\n",
        "                text=certif_text[certif_text.find(\"[\") + 1 : certif_text.rfind(\"]\")].strip(),\n",
        "                dict_keys=[\n",
        "                    \"certif__title\",\n",
        "                    \"certif__organization\",\n",
        "                    \"certif__date\",\n",
        "                    \"certif__expiry_date\",\n",
        "                    \"certif__details\",\n",
        "                ],\n",
        "            )\n",
        "            SKILLS_and_CERTIF[\"CV__Certifications\"] = list_certifs\n",
        "            try:\n",
        "                certif_score_int = int(certif_score[0 : certif_score.rfind(\",\\n\")])\n",
        "            except:\n",
        "                certif_score_int = -1\n",
        "            SKILLS_and_CERTIF[\"Certif__evaluation\"] = {\n",
        "                \"score__certif\": certif_score_int,\n",
        "                \"evaluation__certif\": certif_eval[: certif_eval.rfind(\"}\\n\")].strip()[1:-1],\n",
        "            }\n",
        "\n",
        "    except Exception as exception:\n",
        "        SKILLS_and_CERTIF = {\n",
        "            \"candidate__skills\": [],\n",
        "            \"Skills__evaluation\": {\n",
        "                \"score__skills\": -1,\n",
        "                \"evaluation__skills\": \"unknown\",\n",
        "            },\n",
        "            \"CV__Certifications\": [],\n",
        "            \"Certif__evaluation\": {\n",
        "                \"score__certif\": -1,\n",
        "                \"evaluation__certif\": \"unknown\",\n",
        "            },\n",
        "        }\n",
        "        print(f\"[Error] {exception}\")\n",
        "\n",
        "    return SKILLS_and_CERTIF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "KhVIywx_nYIk",
      "metadata": {
        "id": "KhVIywx_nYIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a428b9-77ab-4009-a8bd-29ab7ff95d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llm  client=<cohere.client.Client object at 0x7cca23516a50> async_client=<cohere.client.AsyncClient object at 0x7cca2f300ad0> model='command-r-plus-08-2024' temperature=0.0 cohere_api_key=SecretStr('**********')\n"
          ]
        }
      ],
      "source": [
        "print(\"llm \",llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "599b93ff-12ce-4e06-a7c2-7a2443abcb67",
      "metadata": {
        "id": "599b93ff-12ce-4e06-a7c2-7a2443abcb67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7786c8-3501-4ca0-e84a-e5a93b6f3841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**15:55:42** \tExtract and evaluate the skills and certifications...\n",
            "prompt, For the following resume delimited by triple backticks, output in english the following information:\n",
            "\n",
            "candidate__skills: Extract the list of soft and hard skills from the skill section. Output a list.\n",
            "The skill section is a separate section.\n",
            "\n",
            "---------\n",
            "\n",
            "Skills__evaluation: Your task is to perform the following actions:\n",
            "1. Rate the quality of the Skills section by giving an integer score from 0 to 100.\n",
            "2. Evaluate (in three sentences and in english) the quality of the Skills section.\n",
            "3. Output a dictionary with the following keys: score__skills, evaluation__skills.\n",
            "\n",
            "---------\n",
            "\n",
            "CV__Certifications: Extraction of all certificates other than education background and academic achievements. For each certificate:\n",
            "1. Extract the title of the certification.\n",
            "2. Extract the name of the organization or institution that issues the certification.\n",
            "3. Extract the date of certification and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "4. Extract the certification expiry date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "5. Extract any other information listed about the certification. if not found, then simply write \"unknown\".\n",
            "6. Output a dictionary with the following keys: certif__title, certif__organization, certif__date, certif__expiry_date, certif__details.\n",
            "\n",
            "Format your response as a list of dictionaries.\n",
            "\n",
            "---------\n",
            "\n",
            "Certif__evaluation: Your task is to perform the following actions:\n",
            "1. Rate the certifications by giving an integer score from 0 to 100.\n",
            "2. Evaluate (in three sentences and in english) the certifications and the quality of the text.\n",
            "3. Format your response as a dictionary with the following keys: score__certif,evaluation__certif.\n",
            "\n",
            "---------\n",
            "\n",
            "For any requested information, if it is not found, output 'unknown'.\n",
            "\n",
            "Format the final output as a json dictionary with the following keys: (candidate__skills, Skills__evaluation, CV__Certifications, Certif__evaluation)\n",
            "\n",
            "Resume: ```[Document(metadata={'source': '/content/data/resume/ChatGPT_dataScientist.pdf', 'doc_number': 0}, page_content='Name Candidate \\nAddress AAA \\ntest@gmail.com \\n123456789 \\n\\nObjective: \\nHighly skilled and experienced data scientist with a track record of success in both Company \\nX and Company Y. Proficient in a range of programming languages and tools, with a strong \\nability to lead teams and adapt to new challenges. Seeking opportunities to leverage \\nexpertise in data science to drive innovation and business growth. \\n\\nExperience: \\nData Scientist - Company Y (2019 - 2024) \\n\\n-  Led a team of data scientists in developing machine learning models for predictive \\n\\nanalytics. \\n\\n-  Utilized TensorFlow and PyTorch for deep learning projects, achieving significant \\n\\n- \\n\\nimprovements in model accuracy. \\nImplemented Spark and Spark MLlib for big data processing, optimizing performance \\nand scalability. \\n\\n-  Conducted data analysis using Tableau and Plotly, generating actionable insights for \\n\\nbusiness stakeholders. \\n\\n-  Collaborated with cross-functional teams to deploy machine learning solutions into \\n\\nproduction environments. \\n\\n-  Mentored junior team members, fostering their professional growth and development. \\n\\nData Scientist - Company X (2014 - 2019) \\n\\n-  Spearheaded data science initiatives, focusing on improving customer engagement \\n\\nand retention. \\n\\n-  Employed Python, Pandas, and Scikit-learn for data preprocessing and predictive \\n\\nmodeling tasks. \\n\\n-  Managed MySQL and Oracle databases, ensuring data integrity and accessibility for \\n\\nanalysis. \\n\\n-  Developed web applications using Flask and HTML to visualize and interact with \\n\\ndata. \\n\\n-  Served as a team leader, coordinating project timelines and priorities to meet \\n\\nbusiness objectives. \\n\\n-  Demonstrated adaptability in rapidly evolving business environments, delivering \\n\\nsolutions on time and within budget. \\n\\nEducation: \\nMaster of Science in Data Science \\nUniversity U1 \\n2014 \\n\\nSkills: \\nProgramming languages: Python, Java \\nData science: TensorFlow, PyTorch, Spark, Spark MLlib, Pandas, Scikit-learn \\nData analysis: Tableau, Plotly \\n\\n \\n \\n \\n \\n \\n\\x0cDatabases: MySQL, Oracle \\nWeb: Flask, HTML \\nSoft skills: Team leadership, Adaptability \\n\\nCertifications: \\nTensorFlow Developer Certificate (2022, Expires: 2025) \\n\\nRecent Projects: \\nChat with Your Data Using Retrieval Augmented Generation \\n\\n- \\n\\nImplemented a conversational AI system using retrieval augmented generation \\ntechniques. \\n\\n-  Leveraged natural language processing models to enable users to interact with data \\n\\nthrough chat interfaces. \\n\\n-  Enhanced user experience and accessibility of data-driven insights. \\n\\nImprove Resume Using the Power of LLM \\n\\n-  Applied large language model (LLM) techniques to optimize and personalize resume \\n\\ncontent. \\n\\n-  Utilized advanced natural language processing algorithms to highlight key skills and \\n\\nachievements effectively. \\n\\n- \\n\\nLanguages: \\nFluent in French, English, and Spanish.')]```\n",
            "response tokens: 341\n",
            "response_content, response_tokens_count {\n",
            "  \"candidate__skills\": [\n",
            "    \"Python\",\n",
            "    \"Java\",\n",
            "    \"TensorFlow\",\n",
            "    \"PyTorch\",\n",
            "    \"Spark\",\n",
            "    \"Spark MLlib\",\n",
            "    \"Pandas\",\n",
            "    \"Scikit-learn\",\n",
            "    \"Tableau\",\n",
            "    \"Plotly\",\n",
            "    \"MySQL\",\n",
            "    \"Oracle\",\n",
            "    \"Flask\",\n",
            "    \"HTML\",\n",
            "    \"Team leadership\",\n",
            "    \"Adaptability\"\n",
            "  ],\n",
            "  \"Skills__evaluation\": {\n",
            "    \"score__skills\": 85,\n",
            "    \"evaluation__skills\": \"The Skills section provides a comprehensive overview of the candidate's technical expertise and soft skills. It covers a wide range of programming languages, data science tools, and data analysis techniques, demonstrating a strong foundation in the field. However, including specific examples or projects where these skills were applied could further enhance the impact of this section.\"\n",
            "  },\n",
            "  \"CV__Certifications\": [\n",
            "    {\n",
            "      \"certif__title\": \"TensorFlow Developer Certificate\",\n",
            "      \"certif__organization\": \"unknown\",\n",
            "      \"certif__date\": \"2022\",\n",
            "      \"certif__expiry_date\": \"2025\",\n",
            "      \"certif__details\": \"unknown\"\n",
            "    }\n",
            "  ],\n",
            "  \"Certif__evaluation\": {\n",
            "    \"score__certif\": 70,\n",
            "    \"evaluation__certif\": \"The Certifications section highlights a specialized credential in TensorFlow development, which is a valuable asset for a data scientist. However, the section could be improved by including more details about the certification, such as the issuing organization and any additional information related to the certification process. Providing a more comprehensive overview of certifications would strengthen the resume.\"\n",
            "  }\n",
            "} 341\n",
            "CPU times: user 28 ms, sys: 2.58 ms, total: 30.6 ms\n",
            "Wall time: 5.58 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'candidate__skills': ['Python',\n",
              "  'Java',\n",
              "  'TensorFlow',\n",
              "  'PyTorch',\n",
              "  'Spark',\n",
              "  'Spark MLlib',\n",
              "  'Pandas',\n",
              "  'Scikit-learn',\n",
              "  'Tableau',\n",
              "  'Plotly',\n",
              "  'MySQL',\n",
              "  'Oracle',\n",
              "  'Flask',\n",
              "  'HTML',\n",
              "  'Team leadership',\n",
              "  'Adaptability'],\n",
              " 'Skills__evaluation': {'score__skills': 85,\n",
              "  'evaluation__skills': \"The Skills section provides a comprehensive overview of the candidate's technical expertise and soft skills. It covers a wide range of programming languages, data science tools, and data analysis techniques, demonstrating a strong foundation in the field. However, including specific examples or projects where these skills were applied could further enhance the impact of this section.\"},\n",
              " 'CV__Certifications': [{'certif__title': 'TensorFlow Developer Certificate',\n",
              "   'certif__organization': 'unknown',\n",
              "   'certif__date': '2022',\n",
              "   'certif__expiry_date': '2025',\n",
              "   'certif__details': 'unknown'}],\n",
              " 'Certif__evaluation': {'score__certif': 70,\n",
              "  'evaluation__certif': 'The Certifications section highlights a specialized credential in TensorFlow development, which is a valuable asset for a data scientist. However, the section could be improved by including more details about the certification, such as the issuing organization and any additional information related to the certification process. Providing a more comprehensive overview of certifications would strengthen the resume.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "SKILLS_and_CERTIF = Extract_Skills_and_Certifications(llm,documents)\n",
        "SKILLS_and_CERTIF"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1901bfd-94ee-40c9-9ddc-0e38c5c55b22",
      "metadata": {
        "id": "e1901bfd-94ee-40c9-9ddc-0e38c5c55b22"
      },
      "source": [
        "## <a class=\"anchor\" id=\"professional\">Work experience and Projects</a>\n",
        "\n",
        "In this section we will extract:\n",
        "1. **work experience:** job title, company and dates.\n",
        "2. **projects:** project title and dates\n",
        "\n",
        "Note that we will not extract job responsibilities or project details in this section to avoid exceeding the output token limit, particularly for `gemini-pro`.\n",
        "\n",
        "We will extract these details in a separate section using our CohereRerank retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "ca373bcc-ebb8-419f-bf84-97567bc0e564",
      "metadata": {
        "id": "ca373bcc-ebb8-419f-bf84-97567bc0e564"
      },
      "outputs": [],
      "source": [
        "def Extract_PROFESSIONAL_EXPERIENCE(llm, documents):\n",
        "    \"\"\"Extract the list of work experience and projects.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response_content, response_tokens_count = invoke_LLM(\n",
        "            llm,\n",
        "            documents,\n",
        "            resume_sections=[\"Work__experience\", \"CV__Projects\"],\n",
        "            info_message=\"Extract list of work experience and projects...\",\n",
        "            language=ASSISTAN_LANGUAGE,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Load response_content into json dictionary\n",
        "            PROFESSIONAL_EXPERIENCE = json.loads(response_content+\"}\", strict=False)\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "            work_experiences = extract_from_text(response_content, '\"Work__experience\": ', '\"CV__Projects\":')\n",
        "            projects = extract_from_text(response_content, '\"CV__Projects\": ', None)\n",
        "\n",
        "            # Create the dictionary\n",
        "            PROFESSIONAL_EXPERIENCE = {}\n",
        "            PROFESSIONAL_EXPERIENCE[\"Work__experience\"] = convert_text_to_list_of_dicts(\n",
        "                text=work_experiences[work_experiences.find(\"[\") + 1 : work_experiences.rfind(\"]\")].strip()[1:-1],\n",
        "                dict_keys=[\"job__title\", \"job__company\", \"job__start_date\", \"job__end_date\"],\n",
        "            )\n",
        "            PROFESSIONAL_EXPERIENCE[\"CV__Projects\"] = convert_text_to_list_of_dicts(\n",
        "                text=projects[projects.find(\"[\") + 1 : projects.rfind(\"]\")].strip()[1:-1],\n",
        "                dict_keys=[\"project__title\", \"project__start_date\", \"project__end_date\"],\n",
        "            )\n",
        "        # delete 'unknown' projects and work experience\n",
        "        try:\n",
        "            for work_experience in PROFESSIONAL_EXPERIENCE[\"Work__experience\"]:\n",
        "                if work_experience[\"job__title\"] == \"unknown\":\n",
        "                    PROFESSIONAL_EXPERIENCE[\"Work__experience\"].remove(work_experience)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        try:\n",
        "            for project in PROFESSIONAL_EXPERIENCE[\"CV__Projects\"]:\n",
        "                if project[\"project__title\"] == \"unknown\":\n",
        "                    PROFESSIONAL_EXPERIENCE[\"CV__Projects\"].remove(project)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    except Exception as exception:\n",
        "        PROFESSIONAL_EXPERIENCE = {\n",
        "            \"Work__experience\": [],\n",
        "            \"CV__Projects\": []\n",
        "        }\n",
        "        print(exception)\n",
        "\n",
        "    return PROFESSIONAL_EXPERIENCE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "8dca3a0c-adee-48c3-8a3c-4227b9f57b65",
      "metadata": {
        "id": "8dca3a0c-adee-48c3-8a3c-4227b9f57b65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6968624-0bc8-4ce8-82f8-01166cc11bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**15:55:47** \tExtract list of work experience and projects...\n",
            "prompt, For the following resume delimited by triple backticks, output in english the following information:\n",
            "\n",
            "Work__experience: Extract all work experiences. For each work experience:\n",
            "1. Extract the job title.\n",
            "2. Extract the company.\n",
            "3. Extract the start date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "4. Extract the end date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "5. Output a dictionary with the following keys: job__title, job__company, job__start_date, job__end_date.\n",
            "\n",
            "Format your response as a list of dictionaries.\n",
            "\n",
            "---------\n",
            "\n",
            "CV__Projects: Include any side projects outside the work experience.\n",
            "For each project:\n",
            "1. Extract the title of the project.\n",
            "2. Extract the start date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "3. Extract the end date and output it in the following format: YYYY/MM/DD or YYYY/MM or YYYY (depending on the availability of the day and month).\n",
            "4. Output a dictionary with the following keys: project__title, project__start_date, project__end_date.\n",
            "\n",
            "Format your response as a list of dictionaries.\n",
            "\n",
            "---------\n",
            "\n",
            "For any requested information, if it is not found, output 'unknown'.\n",
            "\n",
            "Format the final output as a json dictionary with the following keys: (Work__experience, CV__Projects)\n",
            "\n",
            "Resume: ```[Document(metadata={'source': '/content/data/resume/ChatGPT_dataScientist.pdf', 'doc_number': 0}, page_content='Name Candidate \\nAddress AAA \\ntest@gmail.com \\n123456789 \\n\\nObjective: \\nHighly skilled and experienced data scientist with a track record of success in both Company \\nX and Company Y. Proficient in a range of programming languages and tools, with a strong \\nability to lead teams and adapt to new challenges. Seeking opportunities to leverage \\nexpertise in data science to drive innovation and business growth. \\n\\nExperience: \\nData Scientist - Company Y (2019 - 2024) \\n\\n-  Led a team of data scientists in developing machine learning models for predictive \\n\\nanalytics. \\n\\n-  Utilized TensorFlow and PyTorch for deep learning projects, achieving significant \\n\\n- \\n\\nimprovements in model accuracy. \\nImplemented Spark and Spark MLlib for big data processing, optimizing performance \\nand scalability. \\n\\n-  Conducted data analysis using Tableau and Plotly, generating actionable insights for \\n\\nbusiness stakeholders. \\n\\n-  Collaborated with cross-functional teams to deploy machine learning solutions into \\n\\nproduction environments. \\n\\n-  Mentored junior team members, fostering their professional growth and development. \\n\\nData Scientist - Company X (2014 - 2019) \\n\\n-  Spearheaded data science initiatives, focusing on improving customer engagement \\n\\nand retention. \\n\\n-  Employed Python, Pandas, and Scikit-learn for data preprocessing and predictive \\n\\nmodeling tasks. \\n\\n-  Managed MySQL and Oracle databases, ensuring data integrity and accessibility for \\n\\nanalysis. \\n\\n-  Developed web applications using Flask and HTML to visualize and interact with \\n\\ndata. \\n\\n-  Served as a team leader, coordinating project timelines and priorities to meet \\n\\nbusiness objectives. \\n\\n-  Demonstrated adaptability in rapidly evolving business environments, delivering \\n\\nsolutions on time and within budget. \\n\\nEducation: \\nMaster of Science in Data Science \\nUniversity U1 \\n2014 \\n\\nSkills: \\nProgramming languages: Python, Java \\nData science: TensorFlow, PyTorch, Spark, Spark MLlib, Pandas, Scikit-learn \\nData analysis: Tableau, Plotly \\n\\n \\n \\n \\n \\n \\n\\x0cDatabases: MySQL, Oracle \\nWeb: Flask, HTML \\nSoft skills: Team leadership, Adaptability \\n\\nCertifications: \\nTensorFlow Developer Certificate (2022, Expires: 2025) \\n\\nRecent Projects: \\nChat with Your Data Using Retrieval Augmented Generation \\n\\n- \\n\\nImplemented a conversational AI system using retrieval augmented generation \\ntechniques. \\n\\n-  Leveraged natural language processing models to enable users to interact with data \\n\\nthrough chat interfaces. \\n\\n-  Enhanced user experience and accessibility of data-driven insights. \\n\\nImprove Resume Using the Power of LLM \\n\\n-  Applied large language model (LLM) techniques to optimize and personalize resume \\n\\ncontent. \\n\\n-  Utilized advanced natural language processing algorithms to highlight key skills and \\n\\nachievements effectively. \\n\\n- \\n\\nLanguages: \\nFluent in French, English, and Spanish.')]```\n",
            "response tokens: 195\n",
            "[ERROR] json.loads returns error: Extra data: line 28 column 2 (char 681)\n",
            "\n",
            "['INFO'] Parse response content...\n",
            "\n",
            "CPU times: user 21.3 ms, sys: 5.12 ms, total: 26.4 ms\n",
            "Wall time: 3.55 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Work__experience': [{'job__title': 'Data Scientist',\n",
              "   'job__company': 'Company Y',\n",
              "   'job__start_date': '2019',\n",
              "   'job__end_date': '2024'},\n",
              "  {'job__title': 'Data Scientist',\n",
              "   'job__company': 'Company X',\n",
              "   'job__start_date': '2014',\n",
              "   'job__end_date': '2019'}],\n",
              " 'CV__Projects': [{'project__title': 'Chat with Your Data Using Retrieval Augmented Generation',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown'},\n",
              "  {'project__title': 'Improve Resume Using the Power of LLM',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "PROFESSIONAL_EXPERIENCE = Extract_PROFESSIONAL_EXPERIENCE(llm,documents)\n",
        "PROFESSIONAL_EXPERIENCE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ec3331-aaf9-4ab6-92c1-faeda9480cee",
      "metadata": {
        "id": "70ec3331-aaf9-4ab6-92c1-faeda9480cee"
      },
      "source": [
        "## <a class=\"anchor\" id=\"professional_details\">Work experience responsibilities and project details</a>\n",
        "\n",
        "In this section, we will extract work experience responsibilities and project details. We will levarage our CohereRerank retriever to pass only relevant documents to the LLM (instead of all documents)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "e070bab3",
      "metadata": {
        "id": "e070bab3"
      },
      "outputs": [],
      "source": [
        "def get_relevant_documents(query,documents,retriever):\n",
        "    \"\"\"Retreieve most relevant documents from Langchain documents using the CoherRerank retriever.\"\"\"\n",
        "\n",
        "    # 1.1. Get relevant documents using the CohereRerank retriever\n",
        "\n",
        "    retrieved_docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "    # 1.2. Keep only relevant documents where (relevance_score >= (max(relevance_scores) - 0.1))\n",
        "\n",
        "    relevance_scores = [\n",
        "        retrieved_docs[j].metadata[\"relevance_score\"]\n",
        "        for j in range(len(retrieved_docs))\n",
        "    ]\n",
        "    max_relevance_score = max(relevance_scores)\n",
        "    threshold = max_relevance_score - 0.1\n",
        "\n",
        "    relevant_doc_ids = []\n",
        "\n",
        "    for j in range(len(retrieved_docs)):\n",
        "\n",
        "        # Keep relevant documents with (relevance_score >= threshold)\n",
        "        if retrieved_docs[j].metadata[\"relevance_score\"] >= threshold:\n",
        "            relevant_doc_ids.append(retrieved_docs[j].metadata[\"doc_number\"])\n",
        "\n",
        "    # Append the next document to the most relevant document, as relevant information may be split between two documents.\n",
        "    relevant_doc_ids.append(min(relevant_doc_ids[0]+1, len(documents)-1))\n",
        "\n",
        "    relevant_doc_ids = sorted(set(relevant_doc_ids))  # Sort doc ids\n",
        "\n",
        "    # get the most relevant documents (+ next document)\n",
        "    relevant_documents = [\n",
        "        documents[k] for k in relevant_doc_ids\n",
        "    ]\n",
        "\n",
        "    return relevant_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "dee7a6f6-b237-4bb8-8d84-65c171d670d7",
      "metadata": {
        "id": "dee7a6f6-b237-4bb8-8d84-65c171d670d7"
      },
      "outputs": [],
      "source": [
        "##############################################################################\n",
        "#                EXTRACT WORK EXPERIENCE RESPONSIBILITIES\n",
        "##############################################################################\n",
        "\n",
        "def Extract_Job_Responsibilities(llm, documents, retriever, PROFESSIONAL_EXPERIENCE):\n",
        "    \"\"\"Extract job responsibilities for each job in PROFESSIONAL_EXPERIENCE.\"\"\"\n",
        "\n",
        "    now = (datetime.datetime.now()).strftime(\"%H:%M:%S\")\n",
        "    print(f\"**{now}** \\tExtract work experience responsabilities...\")\n",
        "\n",
        "    for i in range(len(PROFESSIONAL_EXPERIENCE[\"Work__experience\"])):\n",
        "        try:\n",
        "            Work_experience_i = PROFESSIONAL_EXPERIENCE[\"Work__experience\"][i]\n",
        "            print(f\"\\n\\n{i}: {Work_experience_i['job__title']}\", end=\" | \")\n",
        "\n",
        "            # 1. Query\n",
        "            query = f\"\"\"Extract from the resume delimited by triple backticks \\\n",
        "all the duties and responsabilities of the following work experience: \\\n",
        "(title = '{Work_experience_i['job__title']}'\"\"\"\n",
        "            if str(Work_experience_i[\"job__company\"]) != \"unknown\":\n",
        "                query += f\" and company = '{Work_experience_i['job__company']}'\"\n",
        "            if str(Work_experience_i[\"job__start_date\"]) != \"unknown\":\n",
        "                query += f\" and start date = '{Work_experience_i['job__start_date']}'\"\n",
        "            if str(Work_experience_i[\"job__end_date\"]) != \"unknown\":\n",
        "                query += f\" and end date = '{Work_experience_i['job__end_date']}'\"\n",
        "            query += \")\\n\"\n",
        "\n",
        "            # 2. For longer CVs (i.e. number of documents > 2),\n",
        "            # use the CohereRerank retriever to find the most relevant documents.\n",
        "            if len(documents)>2:\n",
        "                try:\n",
        "                    relevant_documents = get_relevant_documents(query, documents, retriever)\n",
        "                except Exception as err:\n",
        "                    print(f\"[ERROR] get_relevant_documents error: {err}\")\n",
        "                    relevant_documents = documents\n",
        "            else:\n",
        "                relevant_documents = documents\n",
        "\n",
        "            print(f\"relevant docs : {len(relevant_documents)}\", end=\" | \")\n",
        "\n",
        "            # 3. Invoke the LLM\n",
        "            prompt = (\n",
        "                query\n",
        "                + f\"\"\"Output the duties in a json dictionary with the following keys (__duty_id__,__duty__). \\\n",
        "Use this format: \"1\":\"duty\",\"2\":\"another duty\".\n",
        "Resume:\\n\\n ```{relevant_documents}```\"\"\"\n",
        "            )\n",
        "\n",
        "            print(f\"prompt tokens: {sum(tiktoken_tokens([prompt]))}\", end=\" | \")\n",
        "\n",
        "            response = llm.invoke(prompt)\n",
        "            response_content = response.content[response.content.find(\"{\") : response.content.rfind(\"}\") + 1]\n",
        "            print(f\"\"\"response tokens: {sum(tiktoken_tokens([response_content]))}\"\"\")\n",
        "\n",
        "            try:\n",
        "                # 4. Convert the response content to json dict and update work_experience\n",
        "                Work_experience_i[\"work__duties\"] = json.loads(response_content, strict=False)\n",
        "            except Exception as e:\n",
        "                print(\"\\n[ERROR] json.loads returns error:\", e, \"\\n\\n\")\n",
        "                print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "                Work_experience_i[\"work__duties\"] = {}\n",
        "                list_duties = (\n",
        "                    response_content[response_content.find(\"{\") + 1 : response_content.rfind(\"}\")].strip().split(\",\\n\")\n",
        "                )\n",
        "                for j in range(len(list_duties)):\n",
        "                    try:\n",
        "                        Work_experience_i[\"work__duties\"][f\"{j+1}\"] = (list_duties[j].split('\":')[1].strip()[1:-1])\n",
        "                    except:\n",
        "                        Work_experience_i[\"work__duties\"][f\"{j+1}\"] = \"unknown\"\n",
        "\n",
        "        except Exception as exception:\n",
        "            print(f\"[ERROR] {exception}\")\n",
        "            Work_experience_i[\"work__duties\"] = {}\n",
        "\n",
        "    return PROFESSIONAL_EXPERIENCE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "08cae22a-5c42-495a-872c-acd9b1b4570d",
      "metadata": {
        "id": "08cae22a-5c42-495a-872c-acd9b1b4570d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d524fd-098a-43c5-ec9f-95d331feac31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**15:55:51** \tExtract work experience responsabilities...\n",
            "\n",
            "\n",
            "0: Data Scientist | relevant docs : 1 | prompt tokens: 827 | response tokens: 201\n",
            "\n",
            "[ERROR] json.loads returns error: Extra data: line 4 column 2 (char 135) \n",
            "\n",
            "\n",
            "\n",
            "['INFO'] Parse response content...\n",
            "\n",
            "\n",
            "\n",
            "1: Data Scientist | relevant docs : 1 | prompt tokens: 827 | response tokens: 214\n",
            "\n",
            "[ERROR] json.loads returns error: Extra data: line 4 column 2 (char 136) \n",
            "\n",
            "\n",
            "\n",
            "['INFO'] Parse response content...\n",
            "\n",
            "CPU times: user 38.4 ms, sys: 6.39 ms, total: 44.8 ms\n",
            "Wall time: 6.59 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'job__title': 'Data Scientist',\n",
              "  'job__company': 'Company Y',\n",
              "  'job__start_date': '2019',\n",
              "  'job__end_date': '2024',\n",
              "  'work__duties': {'1': '1',\n",
              "   '2': 'Led a team of data scientists in developing machine learning models for predictive analytics.\"\\n',\n",
              "   '3': '2',\n",
              "   '4': 'Utilized TensorFlow and PyTorch for deep learning projects, improving model accuracy.\"\\n',\n",
              "   '5': '3',\n",
              "   '6': 'Implemented Spark and Spark MLlib for big data processing, optimizing performance and scalability.\"\\n',\n",
              "   '7': '4',\n",
              "   '8': 'Conducted data analysis using Tableau and Plotly, generating business insights.\"\\n',\n",
              "   '9': '5',\n",
              "   '10': 'Collaborated with cross-functional teams to deploy ML solutions into production.\"\\n',\n",
              "   '11': '6',\n",
              "   '12': 'Mentored junior team members, fostering professional development.'}},\n",
              " {'job__title': 'Data Scientist',\n",
              "  'job__company': 'Company X',\n",
              "  'job__start_date': '2014',\n",
              "  'job__end_date': '2019',\n",
              "  'work__duties': {'1': '1',\n",
              "   '2': 'Spearheaded data science initiatives, focusing on improving customer engagement and retention.\"\\n',\n",
              "   '3': '2',\n",
              "   '4': 'Employed Python, Pandas, and Scikit-learn for data preprocessing and predictive modeling tasks.\"\\n',\n",
              "   '5': '3',\n",
              "   '6': 'Managed MySQL and Oracle databases, ensuring data integrity and accessibility for analysis.\"\\n',\n",
              "   '7': '4',\n",
              "   '8': 'Developed web applications using Flask and HTML to visualize and interact with data.\"\\n',\n",
              "   '9': '5',\n",
              "   '10': 'Served as a team leader, coordinating project timelines and priorities to meet business objectives.\"\\n',\n",
              "   '11': '6',\n",
              "   '12': 'Demonstrated adaptability in rapidly evolving business environments, delivering solutions on time and within budget.'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "PROFESSIONAL_EXPERIENCE = Extract_Job_Responsibilities(llm,documents,retriever,PROFESSIONAL_EXPERIENCE)\n",
        "PROFESSIONAL_EXPERIENCE['Work__experience']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "e8c863ca-9a3c-4c39-8d81-ffe60205ff20",
      "metadata": {
        "id": "e8c863ca-9a3c-4c39-8d81-ffe60205ff20"
      },
      "outputs": [],
      "source": [
        "def Extract_Project_Details(llm, documents, retriever, PROFESSIONAL_EXPERIENCE):\n",
        "    \"\"\"Extract project details for each project in PROFESSIONAL_EXPERIENCE.\"\"\"\n",
        "\n",
        "    now = (datetime.datetime.now()).strftime(\"%H:%M:%S\")\n",
        "    print(f\"**{now}** \\tExtract project details...\")\n",
        "\n",
        "    for i in range(len(PROFESSIONAL_EXPERIENCE[\"CV__Projects\"])):\n",
        "        try:\n",
        "            project_i = PROFESSIONAL_EXPERIENCE[\"CV__Projects\"][i]\n",
        "            print(f\"{i}: {project_i['project__title']}\", end=\" | \")\n",
        "\n",
        "            # 1. Extract relevant documents\n",
        "            query = f\"\"\"Extract from the resume (delimited by triple backticks) what is listed about the following project: \\\n",
        "(project title = '{project_i['project__title']}'\"\"\"\n",
        "            if str(project_i[\"project__start_date\"]) != \"unknown\":\n",
        "                query += f\" and start date = '{project_i['project__start_date']}'\"\n",
        "            if str(project_i[\"project__end_date\"]) != \"unknown\":\n",
        "                query += f\" and end date = '{project_i['project__end_date']}'\"\n",
        "            query += \")\"\n",
        "\n",
        "            if len(documents)>2:\n",
        "                try:\n",
        "                    relevant_documents = get_relevant_documents(query, documents, retriever)\n",
        "                except Exception as err:\n",
        "                    print(f\"[ERROR] get_relevant_documents error: {err}\")\n",
        "                    relevant_documents = documents\n",
        "            else:\n",
        "                relevant_documents = documents\n",
        "\n",
        "            print(f\"relevant docs : {len(relevant_documents)}\", end=\" | \")\n",
        "\n",
        "            # 2. Invoke the LLM\n",
        "\n",
        "            prompt = (query + f\"\"\"Format the extracted text into a string (with bullet points).\n",
        "Resume:\\n\\n ```{relevant_documents}```\"\"\" )\n",
        "\n",
        "            print(f\"prompt tokens: {sum(tiktoken_tokens([prompt]))}\", end=\" | \")\n",
        "\n",
        "            response = llm.invoke(prompt)\n",
        "\n",
        "            response_content = response.content\n",
        "            project_i[\"project__description\"] = response_content\n",
        "            print(f\"\"\"response tokens: {sum(tiktoken_tokens([response_content]))}\"\"\")\n",
        "\n",
        "        except Exception as exception:\n",
        "            project_i[\"project__description\"] = \"unknown\"\n",
        "            print(exception)\n",
        "\n",
        "    return PROFESSIONAL_EXPERIENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "d03275e2-b4bb-4c46-a2ca-ebd5caabfa0f",
      "metadata": {
        "id": "d03275e2-b4bb-4c46-a2ca-ebd5caabfa0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c2e970-6e1f-450c-81e0-dd0a47c7c3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**15:55:57** \tExtract project details...\n",
            "0: Chat with Your Data Using Retrieval Augmented Generation | relevant docs : 1 | prompt tokens: 790 | response tokens: 42\n",
            "1: Improve Resume Using the Power of LLM | relevant docs : 1 | prompt tokens: 789 | response tokens: 33\n",
            "CPU times: user 21.4 ms, sys: 247 µs, total: 21.6 ms\n",
            "Wall time: 1.61 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'project__title': 'Chat with Your Data Using Retrieval Augmented Generation',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Implemented a conversational AI system using retrieval augmented generation techniques.\\n- Leveraged natural language processing models to enable users to interact with data through chat interfaces.\\n- Enhanced user experience and accessibility of data-driven insights.'},\n",
              " {'project__title': 'Improve Resume Using the Power of LLM',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Applied large language model (LLM) techniques to optimize and personalize resume content.\\n- Utilized advanced natural language processing algorithms to highlight key skills and achievements effectively.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "PROFESSIONAL_EXPERIENCE = Extract_Project_Details(llm,documents,retriever,PROFESSIONAL_EXPERIENCE)\n",
        "PROFESSIONAL_EXPERIENCE['CV__Projects']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25732a8",
      "metadata": {
        "id": "b25732a8"
      },
      "source": [
        "## <a class=\"anchor\" id=\"improve_workExp\">Improve the work experience section</a>\n",
        "\n",
        "In this section we will improve each bullet point in the work experience responsibilities. The following prompt is passed to the LLM. It contains a placeholder for the language of the assistant and the context (i.e. list of work experience bullet points)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "abb22976-0409-4bbd-ba41-3853714c76d7",
      "metadata": {
        "id": "abb22976-0409-4bbd-ba41-3853714c76d7"
      },
      "outputs": [],
      "source": [
        "PROMPT_IMPROVE_WORK_EXPERIENCE = \"\"\"you are given a work experience text delimited by triple backticks.\n",
        "1. Rate the quality of the work experience text by giving an integer score from 0 to 100.\n",
        "2. Suggest in {language} how to make the work experience text better and stronger.\n",
        "3. Strengthen the work experience text to make it more appealing to a recruiter in {language}. \\\n",
        "Provide additional details on responsibilities and quantify results for each bullet point. \\\n",
        "Format the output as a string.\n",
        "4. Output a json object that contains the following keys: Score__WorkExperience, Comments__WorkExperience, Improvement__WorkExperience.\n",
        "\n",
        "Work experience text: ```{text}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "b8a2126b-a980-43b9-b0bc-1c94770881d9",
      "metadata": {
        "id": "b8a2126b-a980-43b9-b0bc-1c94770881d9"
      },
      "outputs": [],
      "source": [
        "def improve_text_quality(PROMPT,text_to_imporve,llm,language=ASSISTAN_LANGUAGE):\n",
        "    \"\"\"Invoke LLM to improve the text quality.\"\"\"\n",
        "    query = PROMPT.format(text=text_to_imporve,language=language)\n",
        "    response = llm.invoke(query)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "6690b3a9-3a3a-4d23-80ea-85fdd091e8ae",
      "metadata": {
        "id": "6690b3a9-3a3a-4d23-80ea-85fdd091e8ae"
      },
      "outputs": [],
      "source": [
        "def improve_work_experience(WORK_EXPERIENCE:list,llm):\n",
        "    \"\"\"Use LLM to improve each bullet point in the work experience responsibilities.\"\"\"\n",
        "\n",
        "    print(f\"Improve text quality of work Experience...\\n\")\n",
        "\n",
        "    # Call LLM for any work experience to get a better and stronger text.\n",
        "    for i in range(len(WORK_EXPERIENCE)):\n",
        "\n",
        "        WORK_EXPERIENCE_i = WORK_EXPERIENCE[i]\n",
        "\n",
        "        print(f\"{i}: {WORK_EXPERIENCE_i['job__title']}\",end=\" | \")\n",
        "\n",
        "        # 1. Convert the duties from dict to string\n",
        "\n",
        "        text_duties = \"\"\n",
        "        for duty in list(WORK_EXPERIENCE_i['work__duties'].values()):\n",
        "            text_duties += \"- \" + duty\n",
        "\n",
        "        # 2. Invoke the LLM\n",
        "\n",
        "        response = improve_text_quality(PROMPT_IMPROVE_WORK_EXPERIENCE,text_duties,llm)\n",
        "        response_content = response.content\n",
        "\n",
        "        # 3. Convert response content to json dict with keys:\n",
        "        # ('Score__WorkExperience','Comments__WorkExperience','Improvement__WorkExperience')\n",
        "\n",
        "        response_content = response_content[response_content.find(\"{\"):response_content.rfind(\"}\")+1]\n",
        "        print(f\"\"\"response tokens: {sum(tiktoken_tokens([response_content]))}\"\"\", end=\"\\n\")\n",
        "\n",
        "        try:\n",
        "            list_fields = ['Score__WorkExperience','Comments__WorkExperience','Improvement__WorkExperience']\n",
        "            list_rfind = [\",\\n\",\",\\n\",\"\\n\"]\n",
        "            list_exclude_first_car = [False,True,True]\n",
        "            response_content_dict = ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car)\n",
        "            try:\n",
        "                response_content_dict['Score__WorkExperience'] = int(response_content_dict['Score__WorkExperience'])\n",
        "            except:\n",
        "                response_content_dict['Score__WorkExperience'] = -1\n",
        "\n",
        "        except Exception as e:\n",
        "            response_content_dict = {\n",
        "                'Score__WorkExperience':-1,\n",
        "                'Comments__WorkExperience':\"\",\n",
        "                'Improvement__WorkExperience':\"\"}\n",
        "            print(e)\n",
        "\n",
        "\n",
        "        # 4. Update PROFESSIONAL_EXPERIENCE dictionary:\n",
        "\n",
        "        WORK_EXPERIENCE_i['Score__WorkExperience'] = response_content_dict['Score__WorkExperience']\n",
        "        WORK_EXPERIENCE_i['Comments__WorkExperience'] = response_content_dict['Comments__WorkExperience']\n",
        "        WORK_EXPERIENCE_i['Improvement__WorkExperience'] = response_content_dict['Improvement__WorkExperience']\n",
        "\n",
        "    return WORK_EXPERIENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "def08300-07e3-4b3e-86fd-b0f253733f59",
      "metadata": {
        "id": "def08300-07e3-4b3e-86fd-b0f253733f59",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d27e356-294b-408f-8c41-653455aaf989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improve text quality of work Experience...\n",
            "\n",
            "0: Data Scientist | response tokens: 95\n",
            "1: Data Scientist | response tokens: 66\n",
            "CPU times: user 67.5 ms, sys: 5.81 ms, total: 73.3 ms\n",
            "Wall time: 17 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'job__title': 'Data Scientist',\n",
              "  'job__company': 'Company Y',\n",
              "  'job__start_date': '2019',\n",
              "  'job__end_date': '2024',\n",
              "  'work__duties': {'1': '1',\n",
              "   '2': 'Led a team of data scientists in developing machine learning models for predictive analytics.\"\\n',\n",
              "   '3': '2',\n",
              "   '4': 'Utilized TensorFlow and PyTorch for deep learning projects, improving model accuracy.\"\\n',\n",
              "   '5': '3',\n",
              "   '6': 'Implemented Spark and Spark MLlib for big data processing, optimizing performance and scalability.\"\\n',\n",
              "   '7': '4',\n",
              "   '8': 'Conducted data analysis using Tableau and Plotly, generating business insights.\"\\n',\n",
              "   '9': '5',\n",
              "   '10': 'Collaborated with cross-functional teams to deploy ML solutions into production.\"\\n',\n",
              "   '11': '6',\n",
              "   '12': 'Mentored junior team members, fostering professional development.'},\n",
              "  'Score__WorkExperience': 75,\n",
              "  'Comments__WorkExperience': 'The text is a good starting point but lacks specific details and quantifiable achievements. Adding project-specific information, numbers, and percentages will make the work experience more impactful and memorable to recruiters.',\n",
              "  'Improvement__WorkExperience': 'The strengthened text provides a more comprehensive and compelling narrative by including project details, quantifiable results, and a clearer demonstration of leadership and technical skills.'},\n",
              " {'job__title': 'Data Scientist',\n",
              "  'job__company': 'Company X',\n",
              "  'job__start_date': '2014',\n",
              "  'job__end_date': '2019',\n",
              "  'work__duties': {'1': '1',\n",
              "   '2': 'Spearheaded data science initiatives, focusing on improving customer engagement and retention.\"\\n',\n",
              "   '3': '2',\n",
              "   '4': 'Employed Python, Pandas, and Scikit-learn for data preprocessing and predictive modeling tasks.\"\\n',\n",
              "   '5': '3',\n",
              "   '6': 'Managed MySQL and Oracle databases, ensuring data integrity and accessibility for analysis.\"\\n',\n",
              "   '7': '4',\n",
              "   '8': 'Developed web applications using Flask and HTML to visualize and interact with data.\"\\n',\n",
              "   '9': '5',\n",
              "   '10': 'Served as a team leader, coordinating project timelines and priorities to meet business objectives.\"\\n',\n",
              "   '11': '6',\n",
              "   '12': 'Demonstrated adaptability in rapidly evolving business environments, delivering solutions on time and within budget.'},\n",
              "  'Score__WorkExperience': 75,\n",
              "  'Comments__WorkExperience': 'Good technical overview but lacks specific achievements and impact. Needs more details to impress recruiters.',\n",
              "  'Improvement__WorkExperience': \"Enhanced text with quantifiable results and specific examples, showcasing the candidate's leadership and technical skills.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "%%time\n",
        "PROFESSIONAL_EXPERIENCE['Work__experience'] = improve_work_experience(PROFESSIONAL_EXPERIENCE['Work__experience'],llm)\n",
        "\n",
        "PROFESSIONAL_EXPERIENCE['Work__experience']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "d0331d28",
      "metadata": {
        "id": "d0331d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "2361b3a8-636b-4047-913e-d1b939128bee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The strengthened text provides a more comprehensive and compelling narrative by including project details, quantifiable results, and a clearer demonstration of leadership and technical skills."
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "Markdown(PROFESSIONAL_EXPERIENCE['Work__experience'][0]['Improvement__WorkExperience'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14207ffa-4c2b-4700-a8e4-83d08ef2291e",
      "metadata": {
        "id": "14207ffa-4c2b-4700-a8e4-83d08ef2291e"
      },
      "source": [
        ">**Please note that the numbers displayed here are fictitious and generated by AI. Please only report your actual achievements, as no application can determine what you have accomplished.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec7d7465",
      "metadata": {
        "id": "ec7d7465"
      },
      "source": [
        "## <a class=\"anchor\" id=\"improve_projects\">Improve the projects section</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "7533db26",
      "metadata": {
        "id": "7533db26"
      },
      "outputs": [],
      "source": [
        "PROMPT_IMPROVE_PROJECT = \"\"\"you are given a project text delimited by triple backticks.\n",
        "1. Rate the quality of the project text by giving an integer score from 0 to 100.\n",
        "2. Suggest in {language} how to make the project text better and stronger.\n",
        "3. Strengthen the project text to make it more appealing to a recruiter in {language}, \\\n",
        "including the problem, the approach taken, the tools used and quantifiable results. \\\n",
        "Format the output as a string.\n",
        "4. Output a json object with the following keys: Score__project, Comments__project, Improvement__project.\n",
        "\n",
        "project text: ```{text}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "aeec9443",
      "metadata": {
        "id": "aeec9443"
      },
      "outputs": [],
      "source": [
        "def improve_projects(PROJECTS:list,llm):\n",
        "    \"\"\"Use LLM to improve project text.\"\"\"\n",
        "\n",
        "    print(f\"Improve text quality of the projects...\\n\")\n",
        "\n",
        "    for i in range(len(PROJECTS)):\n",
        "\n",
        "        PROJECT_i = PROJECTS[i]\n",
        "\n",
        "        print(f\"{i}: {PROJECTS[i]['project__title']}\",end=\" | \")\n",
        "\n",
        "        # 1. Use the LLM to improve the text quality of each duty\n",
        "        response = improve_text_quality(\n",
        "            PROMPT_IMPROVE_PROJECT,\n",
        "            PROJECT_i['project__title']+\"\\n\"+PROJECT_i['project__description'],\n",
        "            llm\n",
        "        )\n",
        "        response_content = response.content\n",
        "\n",
        "        # 2. Convert response content to json dict with keys:\n",
        "        # ('Score__project','Comments__project','Improvement__project')\n",
        "\n",
        "        response_content = response_content[response_content.find(\"{\"):response_content.rfind(\"}\")+1]\n",
        "        print(f\"\"\"response tokens: {sum(tiktoken_tokens([response_content]))}\"\"\", end=\"\\n\")\n",
        "\n",
        "        try:\n",
        "            list_fields = ['Score__project','Comments__project','Improvement__project']\n",
        "            list_rfind = [\",\\n\",\",\\n\",\"\\n\"]\n",
        "            list_exclude_first_car = [False,True,True]\n",
        "\n",
        "            response_content_dict = ResponseContent_Parser(response_content,list_fields,list_rfind,list_exclude_first_car)\n",
        "            try:\n",
        "                response_content_dict['Score__project'] = int(response_content_dict['Score__project'])\n",
        "            except:\n",
        "                response_content_dict['Score__project'] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            response_content_dict = {\n",
        "                'Score__project':0,\n",
        "                'Comments__project':\"\",\n",
        "                'Improvement__project':\"\"}\n",
        "            print(e)\n",
        "\n",
        "        # 3. Update PROJECTS\n",
        "        PROJECT_i[\"Overall_quality\"] = response_content_dict[\"Score__project\"]\n",
        "        PROJECT_i[\"Comments\"] = response_content_dict[\"Comments__project\"]\n",
        "        PROJECT_i[\"Improvement\"] = response_content_dict[\"Improvement__project\"]\n",
        "\n",
        "    return PROJECTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "6aafcd85",
      "metadata": {
        "id": "6aafcd85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53292e69-258a-4426-95f2-0a0a6f7f4666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improve text quality of the projects...\n",
            "\n",
            "0: Chat with Your Data Using Retrieval Augmented Generation | response tokens: 97\n",
            "1: Improve Resume Using the Power of LLM | response tokens: 89\n",
            "CPU times: user 65.2 ms, sys: 10.3 ms, total: 75.5 ms\n",
            "Wall time: 18.3 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'project__title': 'Chat with Your Data Using Retrieval Augmented Generation',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Implemented a conversational AI system using retrieval augmented generation techniques.\\n- Leveraged natural language processing models to enable users to interact with data through chat interfaces.\\n- Enhanced user experience and accessibility of data-driven insights.',\n",
              "  'Overall_quality': 70,\n",
              "  'Comments': 'The project text is a good starting point but could be enhanced by providing more detail and context. Elaborating on the problem, methodology, and results will make the project stand out and better engage the recruiter.',\n",
              "  'Improvement': 'The improved text includes a clear problem statement, a detailed approach, and quantifiable results, making it more impactful and likely to impress a recruiter.'},\n",
              " {'project__title': 'Improve Resume Using the Power of LLM',\n",
              "  'project__start_date': 'unknown',\n",
              "  'project__end_date': 'unknown',\n",
              "  'project__description': '- Applied large language model (LLM) techniques to optimize and personalize resume content.\\n- Utilized advanced natural language processing algorithms to highlight key skills and achievements effectively.',\n",
              "  'Overall_quality': 70,\n",
              "  'Comments': 'The project has a good foundation but requires more detail and concrete examples to impress recruiters. Expand on the methodology and provide quantifiable results to showcase the impact of your work.',\n",
              "  'Improvement': 'The improved text adds a clear problem statement, elaborates on the technical approach, and includes measurable outcomes, making it more compelling for a recruiter.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "%%time\n",
        "PROFESSIONAL_EXPERIENCE['CV__Projects'] = improve_projects(PROFESSIONAL_EXPERIENCE['CV__Projects'],llm)\n",
        "\n",
        "PROFESSIONAL_EXPERIENCE['CV__Projects']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f663dc",
      "metadata": {
        "id": "d9f663dc"
      },
      "source": [
        "## <a class=\"anchor\" id=\"evaluate_resume\">Evaluate the Resume</a>\n",
        "\n",
        "Here we will evaluate, outline, and analyse the resume's top 3 strengths and top 3 weaknesses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "dc5f4efc-499f-4aa1-a77c-885b2c6312e5",
      "metadata": {
        "id": "dc5f4efc-499f-4aa1-a77c-885b2c6312e5"
      },
      "outputs": [],
      "source": [
        "PROMPT_EVALUATE_RESUME = \"\"\"You are given a resume delimited by triple backticks.\n",
        "1. Provide an overview of the resume in {language}.\n",
        "2. Assess the resume and provide a detailed analysis of its top 3 strengths. \\\n",
        "Format the top 3 strengths as a triple-bulleted string in {language}.\n",
        "3. Assess the resume and provide a detailed analysis of its top 3 weaknesses. \\\n",
        "Format the top 3 weaknesses as a triple-bulleted string in {language}.\n",
        "4. Output a json object that contains the following keys: resume_cv_overview, top_3_strengths, top_3_weaknesses.\n",
        "\n",
        "The strengths and weaknesses lie in the format, style and content of the resume.\n",
        "\n",
        "resume: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(PROMPT_EVALUATE_RESUME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "3e1e13d2-be9a-4980-80dc-63fcc2d59576",
      "metadata": {
        "id": "3e1e13d2-be9a-4980-80dc-63fcc2d59576"
      },
      "outputs": [],
      "source": [
        "def Evaluate_the_Resume(llm,documents):\n",
        "    \"\"\"Evaluate, outline and analyse the resume's top 3 strengths and top 3 weaknesses.\"\"\"\n",
        "    try:\n",
        "        prompt_template = PromptTemplate.from_template(PROMPT_EVALUATE_RESUME)\n",
        "        prompt = prompt_template.format_prompt(text=documents, language=ASSISTAN_LANGUAGE).text\n",
        "\n",
        "        response = llm.invoke(prompt)\n",
        "        response_content = response.content[response.content.find(\"{\") : response.content.rfind(\"}\") + 1]\n",
        "\n",
        "        try:\n",
        "            RESUME_EVALUATION = json.loads(response_content) # Convert response_content to json dictionary\n",
        "        except:\n",
        "            print(\"[ERROR] json.loads returns error:\", e)\n",
        "            print(\"\\n['INFO'] Parse response content...\\n\")\n",
        "            list_fields = [\"resume_cv_overview\", \"top_3_strengths\", \"top_3_weaknesses\"]\n",
        "            list_rfind = [\",\\n\", \",\\n\", \"\\n\"]\n",
        "            list_exclude_first_car = [True, True, True]\n",
        "            RESUME_EVALUATION = ResponseContent_Parser(response_content, list_fields, list_rfind, list_exclude_first_car)\n",
        "\n",
        "    except Exception as error:\n",
        "        RESUME_EVALUATION = {\n",
        "            \"resume_cv_overview\": \"unknown\",\n",
        "            \"top_3_strengths\": \"unknown\",\n",
        "            \"top_3_weaknesses\": \"unknown\",\n",
        "        }\n",
        "        print(f\"An error occured: {error}\")\n",
        "\n",
        "    return RESUME_EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "358882f2-639e-4841-9dcf-c0f8ad8b4771",
      "metadata": {
        "id": "358882f2-639e-4841-9dcf-c0f8ad8b4771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b541ab76-785f-45dd-e817-abadf58d9a98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 40.1 ms, sys: 2.06 ms, total: 42.2 ms\n",
            "Wall time: 11 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'resume_cv_overview': 'The resume presents a well-qualified data scientist with a strong technical background and leadership experience. It highlights their expertise in programming, machine learning, and data analysis, along with successful projects and team management skills.',\n",
              " 'top_3_strengths': ['Diverse technical skill set covering programming languages, data science tools, and machine learning frameworks.',\n",
              "  'Proven leadership abilities, having led teams and mentored juniors in both companies.',\n",
              "  'Demonstrated adaptability through a range of projects, showcasing a versatile data science skill set.'],\n",
              " 'top_3_weaknesses': ['Resume formatting could be improved with a clearer structure and better organization of content.',\n",
              "  'Lacks quantifiable achievements or metrics to showcase the impact of their work.',\n",
              "  'Limited emphasis on soft skills; could provide more examples or details to highlight communication and collaboration abilities.']}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "RESUME_EVALUATION = Evaluate_the_Resume(llm,documents)\n",
        "RESUME_EVALUATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26f25a9f-1501-4318-a14f-512ea326ea16",
      "metadata": {
        "id": "26f25a9f-1501-4318-a14f-512ea326ea16"
      },
      "source": [
        "## <a class=\"anchor\" id=\"recap\">Putting it all together</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "1KZP-wv8Qa0q",
      "metadata": {
        "id": "1KZP-wv8Qa0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d986ef9-5d99-4453-c582-00ad16b0a56a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "df8e6214-89eb-465e-a8e1-4ee778aecdb8",
      "metadata": {
        "id": "df8e6214-89eb-465e-a8e1-4ee778aecdb8"
      },
      "outputs": [],
      "source": [
        "SCANNED_RESUME = {}\n",
        "\n",
        "# 1. Create the dictionary: SCANNED_RESUME\n",
        "for d in [CONTACT_INFORMATION,SUMMARY_EVAL,Education_Language_sections,SKILLS_and_CERTIF,PROFESSIONAL_EXPERIENCE,RESUME_EVALUATION]:\n",
        "    SCANNED_RESUME.update(d)\n",
        "\n",
        "# 2. Save the scanned resume to json file\n",
        "with open('./data/scanned_resume.json', 'w') as fp:\n",
        "    json.dump(SCANNED_RESUME, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "3c696e06-f78d-4476-9b21-1d64211f82fc",
      "metadata": {
        "id": "3c696e06-f78d-4476-9b21-1d64211f82fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b5c2d62-7c9f-4218-959b-6688b173805e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Contact__information': {'candidate__name': 'Name Candidate',\n",
              "  'candidate__title': 'unknown',\n",
              "  'candidate__location': 'AAA',\n",
              "  'candidate__email': 'test@gmail.com',\n",
              "  'candidate__phone': '123456789',\n",
              "  'candidate__social_media': 'unknown',\n",
              "  'evaluation__ContactInfo': \"The resume provides a clear and concise contact section with essential information. The candidate's name, email, and phone number are included, which is sufficient for initial contact purposes. However, the resume could be improved by adding a professional title and a more detailed address to provide a more comprehensive overview of the candidate's location.\",\n",
              "  'score__ContactInfo': 80},\n",
              " 'Summary__evaluation': {'evaluation__summary': \"The summary is well-structured and provides a concise overview of the candidate's profile. It highlights the candidate's expertise in data science, programming languages, and leadership skills. The content is clear and to the point, covering the essential aspects of the candidate's experience and abilities.\",\n",
              "  'score__summary': 85,\n",
              "  'CV__summary_enhanced': \"A seasoned data scientist with 10+ years of experience, I have consistently delivered exceptional results at Company X and Company Y. My technical prowess spans programming languages (Python, Java), machine learning frameworks (TensorFlow, PyTorch), and data analysis tools (Tableau, Plotly). I have successfully led teams, mentored junior members, and adapted to dynamic environments. Notable achievements include developing conversational AI systems and optimizing resume content using LLMs. I aim to secure a position where I can leverage my expertise to drive innovation and contribute to the organization's success.\"},\n",
              " 'CV__summary': 'Highly skilled and experienced data scientist with a track record of success in both Company X and Company Y. Proficient in a range of programming languages and tools, with a strong ability to lead teams and adapt to new challenges. Seeking opportunities to leverage expertise in data science to drive innovation and business growth.',\n",
              " 'CV__Education': [{'edu__college': 'University U1',\n",
              "   'edu__degree': 'Master of Science in Data Science',\n",
              "   'edu__start_date': '2014',\n",
              "   'edu__end_date': 'unknown'}],\n",
              " 'Education__evaluation': {'score__edu': 75,\n",
              "  'evaluation__edu': \"The Education section provides a clear overview of the candidate's academic background, highlighting a Master's degree in Data Science. However, it lacks detail on the specific courses or projects undertaken, which could provide further insight into the candidate's expertise. Including more information about the candidate's academic journey would enhance the overall quality of this section.\"},\n",
              " 'CV__Languages': [{'spoken__language': 'French',\n",
              "   'language__fluency': 'Fluent'},\n",
              "  {'spoken__language': 'English', 'language__fluency': 'Fluent'},\n",
              "  {'spoken__language': 'Spanish', 'language__fluency': 'Fluent'}],\n",
              " 'Languages__evaluation': {'score__language': 100,\n",
              "  'evaluation__language': \"The Languages section is comprehensive and effectively communicates the candidate's language skills. It lists three languages with fluency levels, providing a clear understanding of the candidate's multilingual abilities. This section is well-structured and easy to read, offering valuable information for potential employers.\"},\n",
              " 'candidate__skills': ['Python',\n",
              "  'Java',\n",
              "  'TensorFlow',\n",
              "  'PyTorch',\n",
              "  'Spark',\n",
              "  'Spark MLlib',\n",
              "  'Pandas',\n",
              "  'Scikit-learn',\n",
              "  'Tableau',\n",
              "  'Plotly',\n",
              "  'MySQL',\n",
              "  'Oracle',\n",
              "  'Flask',\n",
              "  'HTML',\n",
              "  'Team leadership',\n",
              "  'Adaptability'],\n",
              " 'Skills__evaluation': {'score__skills': 85,\n",
              "  'evaluation__skills': \"The Skills section provides a comprehensive overview of the candidate's technical expertise and soft skills. It covers a wide range of programming languages, data science tools, and data analysis techniques, demonstrating a strong foundation in the field. However, including specific examples or projects where these skills were applied could further enhance the impact of this section.\"},\n",
              " 'CV__Certifications': [{'certif__title': 'TensorFlow Developer Certificate',\n",
              "   'certif__organization': 'unknown',\n",
              "   'certif__date': '2022',\n",
              "   'certif__expiry_date': '2025',\n",
              "   'certif__details': 'unknown'}],\n",
              " 'Certif__evaluation': {'score__certif': 70,\n",
              "  'evaluation__certif': 'The Certifications section highlights a specialized credential in TensorFlow development, which is a valuable asset for a data scientist. However, the section could be improved by including more details about the certification, such as the issuing organization and any additional information related to the certification process. Providing a more comprehensive overview of certifications would strengthen the resume.'},\n",
              " 'Work__experience': [{'job__title': 'Data Scientist',\n",
              "   'job__company': 'Company Y',\n",
              "   'job__start_date': '2019',\n",
              "   'job__end_date': '2024',\n",
              "   'work__duties': {'1': '1',\n",
              "    '2': 'Led a team of data scientists in developing machine learning models for predictive analytics.\"\\n',\n",
              "    '3': '2',\n",
              "    '4': 'Utilized TensorFlow and PyTorch for deep learning projects, improving model accuracy.\"\\n',\n",
              "    '5': '3',\n",
              "    '6': 'Implemented Spark and Spark MLlib for big data processing, optimizing performance and scalability.\"\\n',\n",
              "    '7': '4',\n",
              "    '8': 'Conducted data analysis using Tableau and Plotly, generating business insights.\"\\n',\n",
              "    '9': '5',\n",
              "    '10': 'Collaborated with cross-functional teams to deploy ML solutions into production.\"\\n',\n",
              "    '11': '6',\n",
              "    '12': 'Mentored junior team members, fostering professional development.'},\n",
              "   'Score__WorkExperience': 75,\n",
              "   'Comments__WorkExperience': 'The text is a good starting point but lacks specific details and quantifiable achievements. Adding project-specific information, numbers, and percentages will make the work experience more impactful and memorable to recruiters.',\n",
              "   'Improvement__WorkExperience': 'The strengthened text provides a more comprehensive and compelling narrative by including project details, quantifiable results, and a clearer demonstration of leadership and technical skills.'},\n",
              "  {'job__title': 'Data Scientist',\n",
              "   'job__company': 'Company X',\n",
              "   'job__start_date': '2014',\n",
              "   'job__end_date': '2019',\n",
              "   'work__duties': {'1': '1',\n",
              "    '2': 'Spearheaded data science initiatives, focusing on improving customer engagement and retention.\"\\n',\n",
              "    '3': '2',\n",
              "    '4': 'Employed Python, Pandas, and Scikit-learn for data preprocessing and predictive modeling tasks.\"\\n',\n",
              "    '5': '3',\n",
              "    '6': 'Managed MySQL and Oracle databases, ensuring data integrity and accessibility for analysis.\"\\n',\n",
              "    '7': '4',\n",
              "    '8': 'Developed web applications using Flask and HTML to visualize and interact with data.\"\\n',\n",
              "    '9': '5',\n",
              "    '10': 'Served as a team leader, coordinating project timelines and priorities to meet business objectives.\"\\n',\n",
              "    '11': '6',\n",
              "    '12': 'Demonstrated adaptability in rapidly evolving business environments, delivering solutions on time and within budget.'},\n",
              "   'Score__WorkExperience': 75,\n",
              "   'Comments__WorkExperience': 'Good technical overview but lacks specific achievements and impact. Needs more details to impress recruiters.',\n",
              "   'Improvement__WorkExperience': \"Enhanced text with quantifiable results and specific examples, showcasing the candidate's leadership and technical skills.\"}],\n",
              " 'CV__Projects': [{'project__title': 'Chat with Your Data Using Retrieval Augmented Generation',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown',\n",
              "   'project__description': '- Implemented a conversational AI system using retrieval augmented generation techniques.\\n- Leveraged natural language processing models to enable users to interact with data through chat interfaces.\\n- Enhanced user experience and accessibility of data-driven insights.',\n",
              "   'Overall_quality': 70,\n",
              "   'Comments': 'The project text is a good starting point but could be enhanced by providing more detail and context. Elaborating on the problem, methodology, and results will make the project stand out and better engage the recruiter.',\n",
              "   'Improvement': 'The improved text includes a clear problem statement, a detailed approach, and quantifiable results, making it more impactful and likely to impress a recruiter.'},\n",
              "  {'project__title': 'Improve Resume Using the Power of LLM',\n",
              "   'project__start_date': 'unknown',\n",
              "   'project__end_date': 'unknown',\n",
              "   'project__description': '- Applied large language model (LLM) techniques to optimize and personalize resume content.\\n- Utilized advanced natural language processing algorithms to highlight key skills and achievements effectively.',\n",
              "   'Overall_quality': 70,\n",
              "   'Comments': 'The project has a good foundation but requires more detail and concrete examples to impress recruiters. Expand on the methodology and provide quantifiable results to showcase the impact of your work.',\n",
              "   'Improvement': 'The improved text adds a clear problem statement, elaborates on the technical approach, and includes measurable outcomes, making it more compelling for a recruiter.'}],\n",
              " 'resume_cv_overview': 'The resume presents a well-qualified data scientist with a strong technical background and leadership experience. It highlights their expertise in programming, machine learning, and data analysis, along with successful projects and team management skills.',\n",
              " 'top_3_strengths': ['Diverse technical skill set covering programming languages, data science tools, and machine learning frameworks.',\n",
              "  'Proven leadership abilities, having led teams and mentored juniors in both companies.',\n",
              "  'Demonstrated adaptability through a range of projects, showcasing a versatile data science skill set.'],\n",
              " 'top_3_weaknesses': ['Resume formatting could be improved with a clearer structure and better organization of content.',\n",
              "  'Lacks quantifiable achievements or metrics to showcase the impact of their work.',\n",
              "  'Limited emphasis on soft skills; could provide more examples or details to highlight communication and collaboration abilities.']}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "# 3. Load the json file (scanned resume)\n",
        "f = open('./data/scanned_resume.json')\n",
        "SCANNED_RESUME = json.load(f)\n",
        "f.close()\n",
        "\n",
        "SCANNED_RESUME"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "362d9d5d-b72c-4aea-9c21-d233cd7d4e00",
      "metadata": {
        "id": "362d9d5d-b72c-4aea-9c21-d233cd7d4e00"
      },
      "source": [
        "## <a class=\"anchor\" id=\"app\">Streamlit application</a>\n",
        "\n",
        "We built a simple Streamlit application. Here is a screenshot..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "3hrDKuO-Qjsm",
      "metadata": {
        "id": "3hrDKuO-Qjsm"
      },
      "outputs": [],
      "source": [
        "!mkdir data/screen_Shots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb883ed-e0b1-40ec-9000-2df9c6fa7a42",
      "metadata": {
        "id": "8fb883ed-e0b1-40ec-9000-2df9c6fa7a42"
      },
      "outputs": [],
      "source": [
        "Image.open(\"./data/screen_Shots/app.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "dKiOV6bzQjB2",
      "metadata": {
        "id": "dKiOV6bzQjB2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8decca92-836f-41d7-8d45-7bf8afbdad4e",
      "metadata": {
        "id": "8decca92-836f-41d7-8d45-7bf8afbdad4e"
      },
      "source": [
        "- On the sidebar, you can select the LLM provider, a model, and adjust its parameters. Additionally, you can choose the assistant language.\n",
        "\n",
        "- To upload your resume in PDF format and analyse it, simply click the 'Analyze resume' button on the main panel. The scanned resume will then be displayed section by section, along with an assessment and improved version of each section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "576db5a8-a450-4d7f-9882-ecc40d9e08a9",
      "metadata": {
        "id": "576db5a8-a450-4d7f-9882-ecc40d9e08a9"
      },
      "source": [
        "## <a class=\"anchor\" id=\"conclusion\">Conclusion</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b9ec899-71ae-4b46-ab5e-4fece3defa52",
      "metadata": {
        "id": "5b9ec899-71ae-4b46-ab5e-4fece3defa52"
      },
      "source": [
        "In this project, we created a CV improver application in Streamlit powered by OpenAI and Google AI APIs.\n",
        "We used Langchain to create a retrieval system that includes a document loader and a CohereRerank retriever. We also utilized its built-in functions to create prompt templates and interact with the LLM.\n",
        "\n",
        "To guide the LLM to the desired output, we leveraged prompt engineering best practices by providing clear and specific instructions, outlining the steps to complete a task, and formatting the output as a JSON structured object.\n",
        "\n",
        "Finally, it is important to note that while LLMs are powerful, no application can accurately determine your accomplishments. AI applications can provide recommendations, but you should only report your actual achievements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wroI-zSDA-03",
      "metadata": {
        "id": "wroI-zSDA-03"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}